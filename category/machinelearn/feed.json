{
    "version": "https://jsonfeed.org/version/1",
    "title": "方家小白 • All posts by \"machinelearn\" category",
    "description": "和你一起遇见更好的自己",
    "home_page_url": "https://fangjiaxiaobai.github.io",
    "items": [
        {
            "id": "https://fangjiaxiaobai.github.io/2021/11/17/machine-learn/Model-evaluation/01-overview/",
            "url": "https://fangjiaxiaobai.github.io/2021/11/17/machine-learn/Model-evaluation/01-overview/",
            "title": "模型评估概述",
            "date_published": "2021-11-17T10:18:00.000Z",
            "content_html": "<h2 id=\"分类\"><a class=\"markdownIt-Anchor\" href=\"#分类\">#</a> 分类</h2>\n<p>模型评估可以分为 离线评估和在线评估 两大类。在产品的不同阶段，我们要根据不同的场景去应用他们。</p>\n<p>两种评估方式由于其场景不同，所评估的关注点也不尽相同。其中，离线评估关注的是模型效果相关的指标，如精准率、 <code>KS</code>  等等。在线评估关注的是业务相关指标，比如新用户的转化率、优惠券的核销率、信贷审核的通过率等等。由于模型的在线评估与业务场景强相关，所以我们的课程重点将放在模型的离线评估上。</p>\n<h3 id=\"在线评估\"><a class=\"markdownIt-Anchor\" href=\"#在线评估\">#</a> 在线评估</h3>\n<p>在线评估是指在模型部署上线后，使用线上真实数据对模型进行的评估。这个时候，产品经理或者运营同学大多采用  <code>ABTest</code>  的方式去判断业务的表现.</p>\n<h3 id=\"离线评估\"><a class=\"markdownIt-Anchor\" href=\"#离线评估\">#</a> 离线评估</h3>\n<p>离线评估是指在模型部署上线前对模型进行的验证和评估工作，这个时候如果发现问题，我们可以很方便地对模型的参数进行调整和改进。</p>\n<p>离线评估又可以分为 <code>特征评估</code> 和 <code>模型评估</code> 两大类。</p>\n<h4 id=\"特征评估\"><a class=\"markdownIt-Anchor\" href=\"#特征评估\">#</a> 特征评估</h4>\n<p>为什么要关注特征评估呢？如果只评估最终模型的指标是否合规的时候，就相当于把模型作为一个 黑盒子了。但同时也要了解模型里面的内容，所以模型特征的评估也是非常重要的。 那特征评估主要关注那些内容呢？</p>\n<h5 id=\"特征自身的稳定性\"><a class=\"markdownIt-Anchor\" href=\"#特征自身的稳定性\">#</a> 特征自身的稳定性</h5>\n<p>对于特征自身的稳定，我们一般使用 <code>PSI</code>  这个指标来判断。  <code>PSI</code>  是评估某个特征的数据随着时间推移发生变化而不再稳定的指标。简单来说，就是看这个特征是不是稳定的，如果一个重要特征不够稳定，就会直接影响到模型整体的稳定性，自然也会影响业务。</p>\n<blockquote>\n<p><code>PSI</code> : ( <code>Population Stability Index</code> .  <code>PSI</code> ), 这里简单介绍一下，后面我会在一篇文章中，详细的介绍 群体稳定性 ( <code>PSI</code> ) 这个概念。 <code>PSI</code>  可用来衡量测试样本及模型开发样本评分的分布差异，为最常见的模型稳定度评估指标。计算公式为:  <code>PSI = sum(（实际占比-预期占比）* ln(实际占比/预期占比))</code> <br>\n 一般以训练集（ <code>INS</code> ）的样本分布作为预期分布，进而跨时间窗按月 / 周来计算 <code>PSI</code> ，得到 <code>Monthly/weekly PSI Report</code> ，进而剔除不稳定的变量。同理，在模型上线部署后，也将通过 <code>PSI</code>  曲线报表来观察模型的稳定性。</p>\n</blockquote>\n<h5 id=\"特征来源的稳定性\"><a class=\"markdownIt-Anchor\" href=\"#特征来源的稳定性\">#</a> 特征来源的稳定性</h5>\n<p>关于 特征来源的稳定性 评估，大致可以分为两种情况:</p>\n<ul>\n<li>如果特征数据来源于集团内部，主要考虑具体来自哪条业务线，这条业务是否稳定，以及业务方是否可能收回或者停止共享数据。</li>\n<li>如果特征接入方是外部公司，特别注意要看这个公司是否合规，是否具备完善的技术储备等等。</li>\n</ul>\n<h5 id=\"成本\"><a class=\"markdownIt-Anchor\" href=\"#成本\">#</a> 成本</h5>\n<p>在获取数据的时候，也要考虑接入的成本问题。</p>\n<ul>\n<li>公司内部数据，一般来说，不存在成本。在不同业务线的角度来说，可能会存在费用分摊的问题。</li>\n<li>外部数据，肯定是有成本的，或许是公司合作，或许是公司直接购买， 正常支付公司费用就好了。特别是注意，如果数据是按调用次数，流量计费的话，是否可以通过预先拉取数据来减少调用。</li>\n</ul>\n<h4 id=\"模型评估\"><a class=\"markdownIt-Anchor\" href=\"#模型评估\">#</a> 模型评估</h4>\n<p>模型的评估主要包括三个部分：统计性、模型性能和模型稳定性。</p>\n<h5 id=\"统计性指标\"><a class=\"markdownIt-Anchor\" href=\"#统计性指标\">#</a> 统计性指标</h5>\n<p>统计性指标指的就是模型输出结果的覆盖度、最大值、最小值、人群分布等指标。我们拿到一个模型，最先看的不是性能指标也不是稳定性，而是统计性指标，它决定了模型到底能不能用。</p>\n<p>在不同的场景下，由于我们的业务不同，对模型的要求不同，对模型统计性指标的关注点也会不同。 对统计性指标进行评估的时候，我们要充分考虑业务场景。</p>\n<p>比如:</p>\n<ul>\n<li>覆盖度。 在金融风控的场景下，如果一个模型的覆盖率低于  <code>60%</code> , 基本上就很难给到客户使用了，因为覆盖低低，风控的业务人员基本没办法对这个模型应用到决策引擎中。如果非要调用的话，最好的情况也就是用到决策树的某个分支上，专门用于某一小部分人群中，不过意义不大。</li>\n<li>最大最小值，也就是分数范围，以信用评分模型为例，如果信用评分模型覆盖的人数很多，但是模型输出的信用分数范围却很窄，假设是 <code>90-95</code> ，很显然，人群并没有好的区分度。 可以参考下芝麻分的范围就设置到了 <code>350-950</code> 。</li>\n<li>人群分布：指的是模型对人打分后，分数和人群的分布形态，这个分布形态应该符合我们的常识，比如用户消费能力评估模型，对于人群的打分结果就应该符合正态分布。</li>\n</ul>\n<h5 id=\"模型性能\"><a class=\"markdownIt-Anchor\" href=\"#模型性能\">#</a> 模型性能</h5>\n<p>模型的性能评估指标是评估模型效果的指标，他和模型要解决的问题相关， 模型要解决的问题，可以分成分类问题和回归问题。</p>\n<h6 id=\"分类模型\"><a class=\"markdownIt-Anchor\" href=\"#分类模型\">#</a> 分类模型</h6>\n<p>分类模型的性能评价指标主要包括：  <code>混淆矩阵</code> ， <code>KS</code> ，  <code>AUC</code>  等等。分类模型的性能评价指标主要包括： <code>混淆矩阵</code> 、 <code>KS</code> 、 <code>AUC</code>  等等。通过混淆矩阵，我们既可以得到一个模型的精确率、召回率这些指标，从而可以评估一个模型的区分能力，我们也可以计算得到的  <code>TPR</code> 、 <code>FPR</code> ，从而计算出  <code>AUC</code> 、 <code>KS</code>  等相关指标。因此，混淆矩阵是评估二分类模型的基础工具。</p>\n<h6 id=\"回归模型\"><a class=\"markdownIt-Anchor\" href=\"#回归模型\">#</a> 回归模型</h6>\n<p>回归模型的性能评价指标主要包括  <code>MAE</code>  (平均绝对误差),  <code>MSE</code>  (均方误差),  <code>RMSE</code>  (均方根误差), <code>R方</code> 。</p>\n<h5 id=\"模型稳定性\"><a class=\"markdownIt-Anchor\" href=\"#模型稳定性\">#</a> 模型稳定性</h5>\n<p>模型的稳定性即判断模型输出结果，是否会随着时间推移，而发生较大变化不再稳定的指标，模型的稳定性会直接影响模型的结果。比如在风控场景下，如果风控模型不够稳定，对于用户风险判断的结果就会发生较大变化。这个时候，我们需要实时调整风控策略，同时也要注意调整后造成决策不合理的情况。对于模型的稳定性，我们主要使用  <code>PSI</code>  进行评估。</p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>希望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/qrcode.jpg\" alt=\"qrcode\"></p>\n",
            "tags": [
                "MachineLearn",
                "模型评估"
            ]
        },
        {
            "id": "https://fangjiaxiaobai.github.io/2021/10/29/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A002-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/",
            "url": "https://fangjiaxiaobai.github.io/2021/10/29/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A002-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/",
            "title": "K-近邻算法(KNN)",
            "date_published": "2021-10-29T10:18:00.000Z",
            "content_html": "<p><code>k-近邻算法</code> ，英文名:  <code>K Nearest Neighbor</code>  算法  又叫 <code>KNN算法</code> ，这个算法是机器学习里面一个比较经典的算法， 总体来说 1 算法是相对比较容易理解的算法.</p>\n<h2 id=\"定义\"><a class=\"markdownIt-Anchor\" href=\"#定义\">#</a> 定义</h2>\n<p>如果一个样本在特征空间中的 k 个最相似 (即特征空间中最邻近) 的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>\n<blockquote>\n<p><code>KNN</code>  算法最早是由 <code>Cover</code>  和 <code>Hart</code>  提出的一种分类算法。应用场景有字符识别、文本分类、图像识别等领域。</p>\n</blockquote>\n<h2 id=\"算法的理解\"><a class=\"markdownIt-Anchor\" href=\"#算法的理解\">#</a> 算法的理解</h2>\n<p>举一个例子来，来分析一下  <code>KNN</code>  算法的实现原理</p>\n<p>假设我们现在有几部电影，如下图:</p>\n<p><img data-src=\"/images/ml/02-knn-1.png\" alt=\"\"></p>\n<p>我们要 根据 搞笑镜头，拥抱镜头，打斗镜头的个数 这三个特征来预测出 《唐人街探案》所属的电影类型.</p>\n<p>我们使用  <code>KNN算法</code>  思想来实现预测。</p>\n<p>将样本中特征作为坐标抽，建立坐标系。从而建立特征空间。本例中，分别把 搞笑镜头，拥抱镜头，打斗镜头 作为 <code>x,y,z</code>  轴。然后把计算出每个样本和 《唐人街探案》 的距离，选择距离最近的前 k 个 ( <code>KNN中的k</code> ) 样本，这 <code>k</code>  个样本大多数所属的电影类别就是 《唐人街探案》的电影类型。</p>\n<blockquote>\n<p>特征空间：是指已特征为坐标轴简历的一种特征的坐标系，可能是多维的。</p>\n</blockquote>\n<p>那你可能对 距离 是如何计算的，有点疑惑。计算距离的方式有很多种。<br>\n先学习一下最简单的 欧氏距离。(初中都学过的！)</p>\n<p>在二维坐标系中， 我们可以使用一下方式来计算出两个点的距离。</p>\n<p><img data-src=\"/images/ml/02-knn-2.png\" alt=\"\"></p>\n<p>在多维的特征空间中，我们也可以使用同样的方式来计算欧式距离。如下图：</p>\n<p><img data-src=\"/images/ml/02-knn-3.png\" alt=\"\"></p>\n<p>那计算一下样本集中的欧式距离，如下图:</p>\n<p><img data-src=\"/images/ml/02-knn-4.png\" alt=\"\"></p>\n<p>并计算出了最近的 5 个样本中有三个喜剧片类型，2 两个爱情片类型。 那根据 <code>KNN</code>  就是喜剧片类型。</p>\n<p>这就是 <code>KNN算法</code> 的核心思想了。</p>\n<p>这个例子中，我们选则了 最近的 <code>5</code>  个样本，也就是 k=5 的时候，会有三个喜剧片类型，两个爱情片类型。这个 <code>5</code>  是如何选择的呢？</p>\n<h2 id=\"k值的选择\"><a class=\"markdownIt-Anchor\" href=\"#k值的选择\">#</a> k 值的选择</h2>\n<p><code>K值</code> 选择问题，李航博士的一书「统计学习方法」上所说：</p>\n<ol>\n<li>\n<p>选择较小的 <code>K</code>  值，就相当于用较小的领域中的训练实例进行预测，“学习” 近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是 “学习” 的估计误差会增大，换句话说， <code>K值</code> 的减小就意味着整体模型变得复杂，容易发生过拟合；</p>\n</li>\n<li>\n<p>选择较大的 <code>K值</code> ，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且 K 值的增大就意味着整体的模型变得简单。</p>\n</li>\n<li>\n<p><code>K=N</code> （ <code>N</code>  为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。</p>\n</li>\n</ol>\n<p>在实际应用中， <code>K值</code> 一般取一个比较小的数值，例如采用交叉验证法 ( <code>cross validation</code> )（简单来说，就是把训练数据在分成两组：训练集和验证集）来选择最优的 K 值。对这个简单的分类器进行泛化，用核方法把这个线性模型扩展到非线性的情况，具体方法是把低维数据集映射到高维特征空间。</p>\n<h2 id=\"knn的优化\"><a class=\"markdownIt-Anchor\" href=\"#knn的优化\">#</a> KNN 的优化</h2>\n<p><code>KNN</code>  算法需要计算所有的样本数据和预测数据的距离，需要选择出距离预测数据最近的 k 个样本数据的预测归类。在庞大的数据量面前，计算所有样本数据距离，显然是不可取的。为了避免每次都重新计算一遍距离， <code>KNN</code>  算法提供了多种优化方法， 比如  <code>KD-tree</code> ,  <code>ball_tree</code> ,  <code>brute</code> . 这几种优化方式的具体实现逻辑，我会在后面的几篇文章中挨个分析。</p>\n<h2 id=\"距离的计算\"><a class=\"markdownIt-Anchor\" href=\"#距离的计算\">#</a> 距离的计算</h2>\n<p><code>KNN</code>  算法，最重要的就是距离。 除了上文提到的欧式距离，还有其他计算距离的方法吗？</p>\n<p>有。</p>\n<p>除了欧式距离，还有 曼哈顿距离，</p>\n<h3 id=\"曼哈顿距离\"><a class=\"markdownIt-Anchor\" href=\"#曼哈顿距离\">#</a> 曼哈顿距离</h3>\n<p>在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是 “曼哈顿距离”。曼哈顿距离也称为 “城市街区距离”( <code>City Block distance</code> )。如下图:<br>\n<img data-src=\"/images/ml/02-knn-6.png\" alt=\"\"></p>\n<p>计算公式见下图:</p>\n<p><img data-src=\"/images/ml/02-knn-7.png\" alt=\"\"></p>\n<h3 id=\"切比雪夫距离-chebyshev-distance\"><a class=\"markdownIt-Anchor\" href=\"#切比雪夫距离-chebyshev-distance\">#</a> 切比雪夫距离 (Chebyshev Distance)</h3>\n<p>国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻 8 个方格中的任意一个。国王从格子 <code>(x1,y1)</code>  走到格子 <code>(x2,y2)</code>  最少需要多少步？这个距离就叫切比雪夫距离。</p>\n<p><img data-src=\"/images/ml/02-knn-8.png\" alt=\"\"></p>\n<p>计算公式见下图:</p>\n<p><img data-src=\"/images/ml/02-knn-9.png\" alt=\"\"></p>\n<h3 id=\"闵可夫斯基距离minkowski-distance\"><a class=\"markdownIt-Anchor\" href=\"#闵可夫斯基距离minkowski-distance\">#</a> 闵可夫斯基距离 (Minkowski Distance)</h3>\n<p>闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。</p>\n<p>两个 n 维变量 <code>a(x11,x12,…,x1n)</code>  与 <code>b(x21,x22,…,x2n)</code>  间的闵可夫斯基距离定义为：</p>\n<p><img data-src=\"/images/ml/02-knn-10.png\" alt=\"\"></p>\n<p>其中 <code>p</code>  是一个变参数：<br>\n当 <code>p=1</code>  时，就是曼哈顿距离；<br>\n当 <code>p=2</code>  时，就是欧氏距离；<br>\n当 <code>p→∞</code> 时，就是切比雪夫距离。</p>\n<p>根据 p 的不同，闵氏距离可以表示某一类 / 种的距离。</p>\n<p>小结：<br>\n1 闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点:<br>\n <code>e.g.</code>  二维样本 (身高 <code>[单位:cm]</code> , 体重 <code>[单位:kg]</code> ), 现有三个样本： <code>a(180,50)</code> ， <code>b(190,50)</code> ， <code>c(180,60)</code> 。<br>\n <code>a</code>  与 <code>b</code>  的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于 <code>a</code>  与 <code>c</code>  的闵氏距离。但实际上身高的 <code>10cm</code>  并不能和体重的 <code>10kg</code>  划等号。</p>\n<p>2 闵氏距离的缺点：<br>\n​ (1) 将各个分量的量纲 ( <code>scale</code> )，也就是 “单位” 相同的看待了；<br>\n​ (2) 未考虑各个分量的分布（期望，方差等）可能是不同的。</p>\n<h3 id=\"标准化欧氏距离-standardized-euclideandistance\"><a class=\"markdownIt-Anchor\" href=\"#标准化欧氏距离-standardized-euclideandistance\">#</a> 标准化欧氏距离 (Standardized EuclideanDistance)</h3>\n<p>标准化欧氏距离是针对欧氏距离的缺点而作的一种改进。</p>\n<p>思路：既然数据各维分量的分布不一样，那先将各个分量都 “标准化” 到均值、方差相等。假设样本集 X 的均值 ( <code>mean</code> ) 为 <code>m</code> ，标准差 ( <code>standard deviation</code> ) 为 <code>s</code> ， <code>X</code>  的 “标准化变量” 表示为：</p>\n<p><img data-src=\"/images/ml/02-knn-11.png\" alt=\"\"></p>\n<p>如果将方差的倒数看成一个权重，也可称之为加权欧氏距离 ( <code>Weighted Euclidean distance</code> )。</p>\n<h3 id=\"余弦距离cosine-distance\"><a class=\"markdownIt-Anchor\" href=\"#余弦距离cosine-distance\">#</a> 余弦距离 (Cosine Distance)</h3>\n<p>几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。</p>\n<p>二维空间中向量 <code>A(x1,y1)</code>  与向量 <code>B(x2,y2)</code>  的夹角余弦公式：<br>\n<img data-src=\"/images/ml/02-knn-12.png\" alt=\"\"></p>\n<p>两个 <code>n</code>  维样本点 <code>a(x11,x12,…,x1n)</code>  和 <code>b(x21,x22,…,x2n)</code>  的夹角余弦为：</p>\n<p><img data-src=\"/images/ml/02-knn-13.png\" alt=\"\"></p>\n<p>即:</p>\n<p><img data-src=\"/images/ml/02-knn-14.png\" alt=\"\"></p>\n<p>夹角余弦取值范围为 <code>[-1,1]</code> 。余弦越大表示两个向量的夹角越小，余弦越小表示两向量的夹角越大。当两个向量的方向重合时余弦取最大值 <code>1</code> ，当两个向量的方向完全相反余弦取最小值 <code>-1</code></p>\n<h3 id=\"汉明距离hamming-distance\"><a class=\"markdownIt-Anchor\" href=\"#汉明距离hamming-distance\">#</a> 汉明距离 (Hamming Distance)</h3>\n<p>两个等长字符串 <code>s1</code>  与 <code>s2</code>  的汉明距离为：将其中一个变为另外一个所需要作的最小字符替换次数。</p>\n<p>例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The Hamming distance between &quot;1011101&quot; and &quot;1001001&quot; is 2. </span><br><span class=\"line\">The Hamming distance between &quot;2143896&quot; and &quot;2233796&quot; is 3. </span><br><span class=\"line\">The Hamming distance between &quot;toned&quot; and &quot;roses&quot; is 3.</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"/images/ml/02-knn-15.png\" alt=\"\"></p>\n<p>汉明重量：是字符串相对于同样长度的零字符串的汉明距离，也就是说，它是字符串中非零的元素个数：对于二进制字符串来说，就是  <code>1</code>  的个数，所以  <code>11101</code>  的汉明重量是  <code>4</code> 。因此，如果向量空间中的元素 <code>a</code>  和 <code>b</code>  之间的汉明距离等于它们汉明重量的差 <code>a-b</code> 。</p>\n<p>应用：汉明重量分析在包括信息论、编码理论、密码学等领域都有应用。比如在信息编码过程中，为了增强容错性，应使得编码间的最小汉明距离尽可能大。但是，如果要比较两个不同长度的字符串，不仅要进行替换，而且要进行插入与删除的运算，在这种场合下，通常使用更加复杂的编辑距离等算法。</p>\n<h3 id=\"杰卡德距离jaccard-distance\"><a class=\"markdownIt-Anchor\" href=\"#杰卡德距离jaccard-distance\">#</a> 杰卡德距离 (Jaccard Distance)</h3>\n<p>杰卡德相似系数 ( <code>Jaccard similarity coefficient</code> )：两个集合 <code>A</code>  和 <code>B</code>  的交集元素在 <code>A</code> ， <code>B</code>  的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号 <code>J(A,B</code> ) 表示：</p>\n<p><img data-src=\"/images/ml/02-knn-16.png\" alt=\"\"></p>\n<p>杰卡德距离 ( <code>Jaccard Distance</code> )：与杰卡德相似系数相反，用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度：</p>\n<p><img data-src=\"/images/ml/02-knn-17.png\" alt=\"\"></p>\n<h3 id=\"马氏距离mahalanobis-distance\"><a class=\"markdownIt-Anchor\" href=\"#马氏距离mahalanobis-distance\">#</a> 马氏距离 (Mahalanobis Distance)</h3>\n<p>下图有两个正态分布图，它们的均值分别为 <code>a</code>  和 <code>b</code> ，但方差不一样，则图中的 <code>A</code>  点离哪个总体更近？或者说 <code>A</code>  有更大的概率属于谁？显然， <code>A</code>  离左边的更近， <code>A</code>  属于左边总体的概率更大，尽管 <code>A</code>  与 <code>a</code>  的欧式距离远一些。这就是马氏距离的直观解释。</p>\n<p><img data-src=\"/images/ml/02-knn-18.png\" alt=\"\"></p>\n<p>马氏距离是基于样本分布的一种距离。</p>\n<p>马氏距离是由印度统计学家马哈拉诺比斯提出的，表示数据的协方差距离。它是一种有效的计算两个位置样本集的相似度的方法。</p>\n<p>与欧式距离不同的是，它考虑到各种特性之间的联系，即独立于测量尺度。</p>\n<p>马氏距离定义：设总体 <code>G</code>  为 <code>m</code>  维总体（考察 <code>m</code>  个指标），均值向量为 <code>μ=（μ1，μ2，… ...，μm，）</code> , 协方差阵为 <code>∑=（σij）</code> ,</p>\n<p>则样本 <code>X=（X1，X2，… …，Xm，）</code> 与总体 G 的马氏距离定义为：</p>\n<p><img data-src=\"/images/ml/02-knn-19.png\" alt=\"\"></p>\n<p>马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为 <code>∑</code> 的随机变量的差异程度：如果协方差矩阵为单位矩阵，马氏距离就简化为欧式距离；如果协方差矩阵为对角矩阵，则其也可称为正规化的欧式距离。</p>\n<h4 id=\"马氏距离特性\"><a class=\"markdownIt-Anchor\" href=\"#马氏距离特性\">#</a> 马氏距离特性：</h4>\n<p>1. 量纲无关，排除变量之间的相关性的干扰；</p>\n<p>2. 马氏距离的计算是建立在总体样本的基础上的，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；</p>\n<p>3 . 计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离计算即可。</p>\n<p>4. 还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如三个样本点 <code>(3,4)</code> ， <code>(5,6)</code> ， <code>(7,8)</code> ，这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。</p>\n<h4 id=\"欧式距离马氏距离\"><a class=\"markdownIt-Anchor\" href=\"#欧式距离马氏距离\">#</a> 欧式距离 &amp; 马氏距离：</h4>\n<p><img data-src=\"/images/ml/02-knn-20.png\" alt=\"\"></p>\n<p>举例：</p>\n<p>已知有两个类 <code>G1</code>  和 <code>G2</code> ，比如 <code>G1</code>  是设备 <code>A</code>  生产的产品， <code>G2</code>  是设备 <code>B</code>  生产的同类产品。设备 <code>A</code>  的产品质量高（如考察指标为耐磨度 <code>X</code> ），其平均耐磨度 <code>μ1=80</code> ，反映设备精度的方差 <code>σ2(1)=0.25</code> ; 设备 B 的产品质量稍差，其平均耐磨损度 <code>μ2=75</code> ，反映设备精度的方差 <code>σ2(2)=4</code> .</p>\n<p>今有一产品 <code>G0</code> ，测的耐磨损度 <code>X0=78</code> ，试判断该产品是哪一台设备生产的？</p>\n<p>直观地看， <code>X0</code>  与 <code>μ1</code> （ <code>设备A</code> ）的绝对距离近些，按距离最近的原则，是否应把该产品判断 <code>设备A</code>  生产的？</p>\n<p>考虑一种相对于分散性的距离，记 <code>X0</code>  与 <code>G1</code> ， <code>G2</code>  的相对距离为 <code>d1</code> ， <code>d2</code> , 则：</p>\n<p><img data-src=\"/images/ml/02-knn-21.png\" alt=\"\"></p>\n<p>因为 <code>d2=1.5 &lt; d1=4</code> ，按这种距离准则，应判断 <code>X0</code>  为设备 B 生产的。</p>\n<p>设备 <code>B</code>  生产的产品质量较分散，出现 <code>X0</code>  为 <code>78</code>  的可能性较大；而 <code>设备A</code>  生产的产品质量较集中，出现 <code>X0</code>  为 <code>78</code>  的可能性较小。</p>\n<p>这种相对于分散性的距离判断就是马氏距离。</p>\n<p><img data-src=\"/images/ml/02-knn-22.png\" alt=\"\"></p>\n<h2 id=\"案例\"><a class=\"markdownIt-Anchor\" href=\"#案例\">#</a> 案例</h2>\n<h3 id=\"预测鸢尾花种类\"><a class=\"markdownIt-Anchor\" href=\"#预测鸢尾花种类\">#</a> 预测鸢尾花种类</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iris_demo</span>():</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.准备数据</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程</span></span><br><span class=\"line\">    <span class=\"comment\"># 3.1 标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器训练(模型训练)</span></span><br><span class=\"line\">    estimator = KNeighborsClassifier(n_neighbors=<span class=\"number\">3</span>)</span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1  方法1：比对真实值和预测值</span></span><br><span class=\"line\">    predict_data = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为: \\n&quot;</span>, predict_data)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;比对真实值和预测值;\\n&quot;</span>, predict_data == y_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2  方法2: 直接计算正确率</span></span><br><span class=\"line\">    score = estimator.score(x_test, y_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;正确率:&quot;</span>, score)</span><br></pre></td></tr></table></figure>\n<h4 id=\"输出结果\"><a class=\"markdownIt-Anchor\" href=\"#输出结果\">#</a> 输出结果</h4>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">预测值为: </span><br><span class=\"line\"> [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2]</span><br><span class=\"line\">比对真实值和预测值;</span><br><span class=\"line\"> [ True  True  True  True  True  True  True  True  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True False  True  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True]</span><br><span class=\"line\">正确率: 0.9666666666666667</span><br></pre></td></tr></table></figure>\n<h4 id=\"使用-gscv-优化\"><a class=\"markdownIt-Anchor\" href=\"#使用-gscv-优化\">#</a> 使用 GSCV 优化</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split, GridSearchCV</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iris_demo</span>():</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.准备数据</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程</span></span><br><span class=\"line\">    <span class=\"comment\"># 3.1 标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器训练(模型训练)</span></span><br><span class=\"line\">    estimator = KNeighborsClassifier()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.1 准备要调的超参数</span></span><br><span class=\"line\">    param_dict = &#123;<span class=\"string\">&quot;n_neighbors&quot;</span>: [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>]&#125;</span><br><span class=\"line\">    <span class=\"comment\"># 4.2 创建 GridSearchCV,使用网格搜索和交叉验证</span></span><br><span class=\"line\">    estimator = GridSearchCV(estimator, param_grid=param_dict, cv=<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1  方法1：比对真实值和预测值</span></span><br><span class=\"line\">    predict_data = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为: \\n&quot;</span>, predict_data)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;比对真实值和预测值;\\n&quot;</span>, predict_data == y_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2  方法2: 直接计算正确率</span></span><br><span class=\"line\">    score = estimator.score(x_test, y_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;正确率:&quot;</span>, score)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 6. 直接查看评估结果哦</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;在交叉验证中验证的最好结果：&quot;</span>, estimator.best_score_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;最好的参数模型：&quot;</span>, estimator.best_estimator_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;每次交叉验证后的准确率结果：\\n&quot;</span>, estimator.cv_results_)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    iris_demo()</span><br></pre></td></tr></table></figure>\n<h4 id=\"输出结果-2\"><a class=\"markdownIt-Anchor\" href=\"#输出结果-2\">#</a> 输出结果</h4>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">预测值为: </span><br><span class=\"line\"> [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0</span><br><span class=\"line\"> 0]</span><br><span class=\"line\">比对真实值和预测值;</span><br><span class=\"line\"> [ True  True  True  True  True  True  True False  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True False  True  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True  True  True  True  True  True  True</span><br><span class=\"line\">  True  True]</span><br><span class=\"line\">正确率: 0.9473684210526315</span><br><span class=\"line\">在交叉验证中验证的最好结果： 0.9732100521574205</span><br><span class=\"line\">最好的参数模型： KNeighborsClassifier()</span><br><span class=\"line\">每次交叉验证后的准确率结果：</span><br><span class=\"line\"> &#123;&#x27;mean_fit_time&#x27;: array([0.0008928 , 0.00045244, 0.00044529]), &#x27;std_fit_time&#x27;: array([5.74547103e-04, 5.05512361e-06, 2.92218150e-06]), &#x27;mean_score_time&#x27;: array([0.00226967, 0.00184425, 0.00182239]), &#x27;std_score_time&#x27;: array([6.28895378e-04, 2.09757168e-05, 1.41269575e-05]), &#x27;param_n_neighbors&#x27;: masked_array(data=[1, 3, 5],</span><br><span class=\"line\">             mask=[False, False, False],</span><br><span class=\"line\">       fill_value=&#x27;?&#x27;,</span><br><span class=\"line\">            dtype=object), &#x27;params&#x27;: [&#123;&#x27;n_neighbors&#x27;: 1&#125;, &#123;&#x27;n_neighbors&#x27;: 3&#125;, &#123;&#x27;n_neighbors&#x27;: 5&#125;], &#x27;split0_test_score&#x27;: array([0.97368421, 0.97368421, 0.97368421]), &#x27;split1_test_score&#x27;: array([0.97297297, 0.97297297, 0.97297297]), &#x27;split2_test_score&#x27;: array([0.94594595, 0.89189189, 0.97297297]), &#x27;mean_test_score&#x27;: array([0.96420104, 0.94618303, 0.97321005]), &#x27;std_test_score&#x27;: array([0.01291157, 0.03839073, 0.00033528]), &#x27;rank_test_score&#x27;: array([2, 3, 1], dtype=int32)&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"预测facebook签到位置\"><a class=\"markdownIt-Anchor\" href=\"#预测facebook签到位置\">#</a> 预测 facebook 签到位置</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">facebook_demo</span>():</span></span><br><span class=\"line\">    <span class=\"comment\"># 准备数据</span></span><br><span class=\"line\">    data = pd.read_csv(<span class=\"string\">&#x27;./train.csv&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(data.head())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">## 2.1 处理时间特征</span></span><br><span class=\"line\">    time = pd.to_datetime(data[<span class=\"string\">&#x27;time&#x27;</span>], unit=<span class=\"string\">&#x27;s&#x27;</span>)</span><br><span class=\"line\">    time = pd.DatetimeIndex(time)</span><br><span class=\"line\"></span><br><span class=\"line\">    data[<span class=\"string\">&#x27;hour&#x27;</span>] = time.hour</span><br><span class=\"line\">    data[<span class=\"string\">&#x27;day&#x27;</span>] = time.day</span><br><span class=\"line\">    data[<span class=\"string\">&#x27;weekday&#x27;</span>] = time.weekday</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(data.head(<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.2.去掉签到少的地方</span></span><br><span class=\"line\">    place_count = data.groupby(<span class=\"string\">&quot;place_id&quot;</span>).count()</span><br><span class=\"line\">    place_count = place_count[place_count[<span class=\"string\">&quot;row_id&quot;</span>] &gt; <span class=\"number\">3</span>]</span><br><span class=\"line\">    data = data[data[<span class=\"string\">&quot;place_id&quot;</span>].isin(place_count.index)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.3 确定特征和目标值</span></span><br><span class=\"line\">    x = data[[<span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;y&#x27;</span>, <span class=\"string\">&quot;accuracy&quot;</span>, <span class=\"string\">&quot;day&quot;</span>, <span class=\"string\">&quot;hour&quot;</span>, <span class=\"string\">&quot;weekday&quot;</span>]]</span><br><span class=\"line\">    y = data[[<span class=\"string\">&#x27;place_id&#x27;</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.4 拆分数据集</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3. 特征处理</span></span><br><span class=\"line\">    <span class=\"comment\"># 3.1 标准化处理</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器学习</span></span><br><span class=\"line\">    <span class=\"comment\"># 4.1 实例化估计器</span></span><br><span class=\"line\">    estimator = KNeighborsClassifier()</span><br><span class=\"line\">    param_dict = &#123;<span class=\"string\">&#x27;neighbors&#x27;</span>: [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>]&#125;</span><br><span class=\"line\">    estimator = GridSearchCV(estimator=estimator, param_grid=param_dict)</span><br><span class=\"line\">    <span class=\"comment\"># 4.2 模型训练</span></span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 模型评估</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n最后预测的准确率为: &quot;</span>, estimator.score(x_test, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n在交叉验证中验证的最好结果:\\n&quot;</span>, estimator.best_score_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n最好的参数模型:\\n&quot;</span>, estimator.best_estimator_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n每次交叉验证后的验证集准确率结果和训练集准确率结果:\\n&quot;</span>, estimator.cv_results_)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    facebook_demo()</span><br></pre></td></tr></table></figure>\n<p>本案例来自  <code>Kaggle</code>  的题目，感兴趣的朋友可以登录:<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9uYXZvc2h0YS9ncmlkLWtubi9zY3JpcHQ=\">https://www.kaggle.com/navoshta/grid-knn/script</span>  查看</p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>希望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/ml/qrcode.jpg\" alt=\"qrcode\"></p>\n",
            "tags": [
                "MachineLearn",
                "KNN"
            ]
        },
        {
            "id": "https://fangjiaxiaobai.github.io/2021/10/28/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A001-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BA%86%E8%A7%A3/",
            "url": "https://fangjiaxiaobai.github.io/2021/10/28/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A001-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BA%86%E8%A7%A3/",
            "title": "机器学习-简介",
            "date_published": "2021-10-28T10:18:00.000Z",
            "content_html": "<h2 id=\"概述\"><a class=\"markdownIt-Anchor\" href=\"#概述\">#</a> 概述</h2>\n<h3 id=\"什么是机器学习\"><a class=\"markdownIt-Anchor\" href=\"#什么是机器学习\">#</a> 什么是机器学习</h3>\n<p>从历史数据中自动分析获得规律 (模型), 并利用规律对未知数据进行预测。</p>\n<h3 id=\"为什么需要机器学习\"><a class=\"markdownIt-Anchor\" href=\"#为什么需要机器学习\">#</a> 为什么需要机器学习</h3>\n<p>解放生产力：智能客服<br>\n解决专业问题: ET 医疗<br>\n提供社会便利：提供社会便利</p>\n<h3 id=\"机器学习的应用场景\"><a class=\"markdownIt-Anchor\" href=\"#机器学习的应用场景\">#</a> 机器学习的应用场景</h3>\n<p>方方面面</p>\n<h2 id=\"机器学习的工作流程\"><a class=\"markdownIt-Anchor\" href=\"#机器学习的工作流程\">#</a> 机器学习的工作流程</h2>\n<p>获取数据 -&gt; 数据基本处理 -&gt; 特征工程 -&gt; 机器学习 (模型训练) -&gt; 模型评估。</p>\n<p>评估符合要求，则上线服务。如果不符合要求，则重复上述步骤。</p>\n<h2 id=\"数据集的结构\"><a class=\"markdownIt-Anchor\" href=\"#数据集的结构\">#</a> 数据集的结构</h2>\n<p>机器学习的数据大部分数据存储到文件中。数据库中间件 (mysql,nosql) 等性能瓶颈，读取速度，格式不符合机器学习要求的数据格式。<br>\nnumpy 非常快，因为什么？释放了 GIL。</p>\n<p>在数据集中：<br>\n一行数据成为一个样本。<br>\n一列数据称为一个特征。</p>\n<h3 id=\"可用的数据集\"><a class=\"markdownIt-Anchor\" href=\"#可用的数据集\">#</a> 可用的数据集</h3>\n<p>Kaggle: 大数据竞赛平台，80 万科学家，真实数据，数据量巨大 <span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9kYXRhc2V0cw==\">https://www.kaggle.com/datasets</span></p>\n<p>UCI: 收录了 360 个数据集。覆盖科学，生活经济等领域，数据量几十万 <span class=\"exturl\" data-url=\"aHR0cDovL2FyY2hpdmUuaWNzLnVjaS5lZHUvbWwvaW5kZXgucGhw\">http://archive.ics.uci.edu/ml/index.php</span></p>\n<p>scikit-learn: 数据量小，方便学习。<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zY2lraXQtbGVhcm4ub3JnL3N0YWJsZS8saHR0cHM6Ly9zY2lraXQtbGVhcm4ub3JnL3N0YWJsZS9zdGFibGUvZGF0YXNldHMvaW5kZXguaHRtbA==\">https://scikit-learn.org/stable/,https://scikit-learn.org/stable/stable/datasets/index.html</span></p>\n<h3 id=\"常用数据集数据的结构组成\"><a class=\"markdownIt-Anchor\" href=\"#常用数据集数据的结构组成\">#</a> 常用数据集数据的结构组成</h3>\n<ul>\n<li>特征值 + 目标值 (目标值是练习的和离散的)。特征？比如分辨男女，一个人身高体重皮肤颜色，头发长度。都是特征值。目标值：这个人是男是女，就是目标值。</li>\n<li>只有特征值，没有目标值</li>\n</ul>\n<h3 id=\"数据分割\"><a class=\"markdownIt-Anchor\" href=\"#数据分割\">#</a> 数据分割</h3>\n<p>机器学习一般的数据集会划分为两个部分。</p>\n<ul>\n<li>训练数据：用于训练，构建模型</li>\n<li>测试数据：在数据检验时使用，用于评估模型是否有效、</li>\n</ul>\n<p>划分比例：</p>\n<ul>\n<li>训练集： 79% 80% 75%</li>\n<li>测试集： 30%， 20% 25%</li>\n</ul>\n<h2 id=\"数据的特征工程\"><a class=\"markdownIt-Anchor\" href=\"#数据的特征工程\">#</a> 数据的特征工程</h2>\n<h3 id=\"数据特征是什么\"><a class=\"markdownIt-Anchor\" href=\"#数据特征是什么\">#</a> 数据特征是什么</h3>\n<p>将原始数据转换为更好的代表预测模型的潜在问题的特征的过程，从而提高对未知数据的预测准确性。</p>\n<p>使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发送更好的作用的过程。</p>\n<ul>\n<li>\n<p>特征提取： 将任意数据 (文本或者图像) 转换为可用于机器学习的数字特征。</p>\n</li>\n<li>\n<p>特征预处理：通过转化函数将特征数据转换成更加适合算法模型的特征数据过程。</p>\n<ul>\n<li>归一化：为什么要进行归一化？特征的单位或者大小相差较大，或者某特征的方法相比其他特征要大出几个数量级，容易影响 (支配) 目标结果，使得算法无法学习到其他的特征。\n<ul>\n<li>归一化的定义： 通过对原始数据进行变换，把数据映射到默认 <code>[0,1]</code>  之间。<br>\n计算公式:<br>\n<img data-src=\"./images/%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>标准化：通过对原始数据进行把数据变换到均值为 0，标准差为 1 的范围内。<br>\n<img data-src=\"./images/%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"\"><br>\n对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变<br>\n对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。</li>\n</ul>\n</li>\n<li>\n<p>特征降维：指在限定条件下，降低随机变量 (特征) 个数，得到一组 &quot;不相关&quot; 主变量的过程。<br>\n减少特征的数量，同时保留原来数据的大部分信息。可以通过 PCA 算法来是实现特征降维。</p>\n</li>\n</ul>\n<p>为什么要降维呢？随着数据维度不断降低，数据存储所需的空间也会随之减少。低维数据有助于减少计算 / 训练用时。一些算法在高维度数据上容易表现不佳，降维可提高算法可用性。降维可以用删除冗余特征解决多重共线性问题。比如我们有两个变量：“一段时间内在跑步机上的耗时” 和 “卡路里消耗量”。这两个变量高度相关，在跑步机上花的时间越长，燃烧的卡路里自然就越多。因此，同时存储这两个数据意义不大，只需一个就够了。降维有助于数据可视化。如前所述，如果数据维度很高，可视化会变得相当困难，而绘制二维三维数据的图表非常简单。</p>\n<h3 id=\"特征工程的意义\"><a class=\"markdownIt-Anchor\" href=\"#特征工程的意义\">#</a> 特征工程的意义</h3>\n<p>直接影响预测结果。</p>\n<h4 id=\"特征抽取实例演示\"><a class=\"markdownIt-Anchor\" href=\"#特征抽取实例演示\">#</a> 特征抽取实例演示</h4>\n<h4 id=\"特征抽取api\"><a class=\"markdownIt-Anchor\" href=\"#特征抽取api\">#</a> 特征抽取 API</h4>\n<p>字典特征抽取类:   <code>sklearn.feature_extraction.DictVectorizer</code> <br>\n 作用：对字典进行特征值化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DictVectorizer(sparse=<span class=\"literal\">True</span>,....)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># X: 字典或者包含字典的迭代器</span></span><br><span class=\"line\"><span class=\"comment\"># 返回值: 返回sparse矩阵</span></span><br><span class=\"line\">DictVectorizer.fit_transform(X):</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># X: array数组或者sparse矩阵</span></span><br><span class=\"line\"><span class=\"comment\"># 返回值: 转换之前的数据格式</span></span><br><span class=\"line\">DictVectorizer.invers_transform(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 返回类别名称</span></span><br><span class=\"line\">DictVectorizer.get_feature_names()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 按照原先的标准转换</span></span><br><span class=\"line\">DictVectorizerr.transform(X)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h5 id=\"使用流程\"><a class=\"markdownIt-Anchor\" href=\"#使用流程\">#</a> 使用流程</h5>\n<p>1. 实例化类 DictVectorizer<br>\n2. 调用 fit_transform 方法进行特征抽取</p>\n<h4 id=\"字典数据提取的现象\"><a class=\"markdownIt-Anchor\" href=\"#字典数据提取的现象\">#</a> 字典数据提取的现象:</h4>\n<p>把字典中国的一些类别数据，分别进行转换特征数据。</p>\n<blockquote>\n<p>文本特征抽取 Count</p>\n</blockquote>\n<h4 id=\"作用-对文本数据进行特征值化\"><a class=\"markdownIt-Anchor\" href=\"#作用-对文本数据进行特征值化\">#</a> 作用：对文本数据进行特征值化</h4>\n<h4 id=\"类-sklearnfeature_extractiontextcountvectorizer\"><a class=\"markdownIt-Anchor\" href=\"#类-sklearnfeature_extractiontextcountvectorizer\">#</a> 类: sklearn.feature_extraction.text.CountVectorizer</h4>\n<h4 id=\"api\"><a class=\"markdownIt-Anchor\" href=\"#api\">#</a> API</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 返回的是词频矩阵</span></span><br><span class=\"line\">CountVectorizer()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 统计所有文章中出现的词的位置标识，对应着列表中的单词(单个字母不统计,不支持中文,需要使用jieba分词)，</span></span><br><span class=\"line\">    <span class=\"comment\"># X 文本或者包含文本字符串的可迭代对象</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 返回sparse矩阵</span></span><br><span class=\"line\">    CountVectorizer.fit_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># X： array数据或者sparse矩阵</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值：转换之前的数据格式</span></span><br><span class=\"line\">    CountVetorizer.inverse_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 单词列表</span></span><br><span class=\"line\">    CountVetorizer.get_feature_names()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>文本特征抽取 tfidf</p>\n</blockquote>\n<p>主要思想是： 如果某个词或短语在一篇文章中出现的概率高，b 并且在其他文章中很少出现，则认为该词或者短语具有很好的类别区分能力，适合用来分类。</p>\n<p>TF-IDF 的作用：  用以评估一字词对于一个文件集成一个语料库中的其中一份文件的重要程度。</p>\n<p>tf (term frequency 词的频率)<br>\n idf (inverse document frequency 逆文档频率) log (总文档数据 / 该词出现的文档数量)</p>\n<p>tf * idf 这个值称为重要性。</p>\n<h4 id=\"类-sklearnfeature_extractiontexttfidfvectorizer\"><a class=\"markdownIt-Anchor\" href=\"#类-sklearnfeature_extractiontexttfidfvectorizer\">#</a> 类  <code>sklearn.feature_extraction.text.TfidfVectorizer </code></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 返回的是词频矩阵</span></span><br><span class=\"line\">CountVectorizer(stop_worlds=N)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 统计所有文章中出现的词的位置标识，对应着列表中的单词(单个字母不统计,不支持中文,需要使用jieba分词)，</span></span><br><span class=\"line\">    <span class=\"comment\"># X 文本或者包含文本字符串的可迭代对象</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 返回sparse矩阵</span></span><br><span class=\"line\">    CountVectorizer.fit_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># X： array数据或者sparse矩阵</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值：转换之前的数据格式</span></span><br><span class=\"line\">    CountVetorizer.inverse_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 单词列表</span></span><br><span class=\"line\">    CountVetorizer.get_feature_names()</span><br></pre></td></tr></table></figure>\n<h4 id=\"为什么需要这\"><a class=\"markdownIt-Anchor\" href=\"#为什么需要这\">#</a> 为什么需要这</h4>\n<p>分类机器学习算法的应用基础。</p>\n<h4 id=\"应用\"><a class=\"markdownIt-Anchor\" href=\"#应用\">#</a> 应用</h4>\n<ul>\n<li>文本分类</li>\n<li>情感分析</li>\n</ul>\n<h4 id=\"数据的预处理\"><a class=\"markdownIt-Anchor\" href=\"#数据的预处理\">#</a> 数据的预处理</h4>\n<p>通过特定的统计方法 (数学方法), 将数据转换成算法要求的数据。</p>\n<h5 id=\"数据处理的方法\"><a class=\"markdownIt-Anchor\" href=\"#数据处理的方法\">#</a> 数据处理的方法</h5>\n<p>API:<br>\n 在 sklearn.preprocessing 中。</p>\n<ul>\n<li>\n<p>数值型数据:</p>\n<ul>\n<li>\n<p>标准缩放</p>\n<ul>\n<li>\n<p>归一化:<br>\n 通过对原始数据机型交换把数据映射到 (默认 [0,1]) 之间<br>\n<img data-src=\"./images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%BD%92%E4%B8%80%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"机器学习_归一归一化公式.png\"><br>\n <code> API: MinMaxScalar</code> <br>\n 实现归一化的步骤：</p>\n<ul>\n<li>实例化 <code>MinMaxScalar</code></li>\n<li>调用  <code>fit_transform</code>  进行转换</li>\n</ul>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mm = MinMaxScalar(feature_range=(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">data = mm.fit_transform([[],[],[]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>标准化<br>\n通过对原始数据进行变换把数据变换到均值为 0，标准差为 1 的范围内。<br>\n<img data-src=\"./images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"机器学习_标准化公式\"><br>\n对于归一化来讲，如果出现了异常点，影响了最大值和最小值，那么结果显然会发生改变.<br>\n 对标准化来讲，如果出现异常点，由于具有一定数据量，少量的异常点对平均值的影响并不大，从而方差改变较小。</p>\n<p><code>API: StandardScalar(....) </code> <br>\n标准化的步骤：</p>\n<ul>\n<li>实例化 StandardScalar</li>\n<li>调用 fit_transfrom</li>\n</ul>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#处理之后每列来说所有数据都聚集在均值0附近标准差差为1</span></span><br><span class=\"line\">StandardScalar(...)</span><br><span class=\"line\">    <span class=\"comment\"># X:numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 转换后的形状相同的array</span></span><br><span class=\"line\">    StandardScalar.fit_transfrom(X)</span><br><span class=\"line\">    <span class=\"comment\"># 原始数据中每列特征的平均值</span></span><br><span class=\"line\">    StandardScalar.mean_</span><br><span class=\"line\">    <span class=\"comment\"># 原始数据每列特征的方差</span></span><br><span class=\"line\">    StandardScalar.std_</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>缺失值<br>\n处理缺失值的方法:</p>\n<ul>\n<li>删除</li>\n<li>插补</li>\n</ul>\n</li>\n</ul>\n<p>插补使用的是:  <code>Imputer</code>  方法</p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 完成缺失值插补</span></span><br><span class=\"line\">Imputer(miss_values=<span class=\"string\">&#x27;NaN&#x27;</span>,strategy=<span class=\"string\">&#x27;mean&#x27;</span>,axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># X: numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 转换后的形状相同的array</span></span><br><span class=\"line\">    Imputer.fit_transform(X)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li>\n<p>类别型数据</p>\n<ul>\n<li>one-hot 编码</li>\n</ul>\n</li>\n<li>\n<p>时间类型</p>\n<ul>\n<li>时间的拆分</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数据的降维\"><a class=\"markdownIt-Anchor\" href=\"#数据的降维\">#</a> 数据的降维</h3>\n<p>降维指的是降的是特征的维度 (特征的数量)。<br>\n两种方式：特征选择 和 主成分分析</p>\n<h4 id=\"特征选择的原因\"><a class=\"markdownIt-Anchor\" href=\"#特征选择的原因\">#</a> 特征选择的原因</h4>\n<ul>\n<li>冗余：部分特征的相关度高，容易消耗计算机性能</li>\n<li>噪声：部分特征会对结果产生影响</li>\n</ul>\n<h4 id=\"特征选择是什么\"><a class=\"markdownIt-Anchor\" href=\"#特征选择是什么\">#</a> 特征选择是什么？</h4>\n<p>就是单纯的从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前后可以改变值，也可以不改变值。但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中一部分特征。<br>\n主要方法有:</p>\n<ul>\n<li>filter (过滤式): varianceThreshold</li>\n<li>Embedde (嵌入式): 正则化，决策树</li>\n<li>Wrapper (包裹式)</li>\n<li>神经网络</li>\n</ul>\n<p>API:  <code> sklearn.feature_selection.VarianceThreshold</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 删除所有低方差特征</span></span><br><span class=\"line\">VarianceThreshold(threhold=<span class=\"number\">0.0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># X: numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 训练集差异地域threshold的特征将被删除</span></span><br><span class=\"line\">    <span class=\"comment\"># 默认值是保留所有非零方差特征,即删除所有样本中具有相同值的特征</span></span><br><span class=\"line\">    Variance.fit_transform(X)</span><br></pre></td></tr></table></figure>\n<h3 id=\"pca-分析简化数据集的技术\"><a class=\"markdownIt-Anchor\" href=\"#pca-分析简化数据集的技术\">#</a> PCA 分析简化数据集的技术</h3>\n<p>目的是： 数据维度压缩，尽可能降低原数据的维度 (复杂度), 损失少量信息<br>\n作用：可以削减回归分析或者聚类分析中特征的数量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将数据分解为较低维数的空间</span></span><br><span class=\"line\">PCA(n_components=<span class=\"literal\">None</span>)</span><br><span class=\"line\">    <span class=\"comment\"># X: numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 转化后制定维度的array</span></span><br><span class=\"line\">    PCA.fit_transform(X)</span><br></pre></td></tr></table></figure>\n<h2 id=\"机器学习算法基础\"><a class=\"markdownIt-Anchor\" href=\"#机器学习算法基础\">#</a> 机器学习算法基础</h2>\n<h3 id=\"监督学习supervised-learnning\"><a class=\"markdownIt-Anchor\" href=\"#监督学习supervised-learnning\">#</a> 监督学习：supervised learnning</h3>\n<p>输入数据由输入特征值 和 目标值组成。</p>\n<p>目标值连续则是回归问题， 目标值离散则是分类问题。</p>\n<h3 id=\"无监督学习-unsupervised-learning\"><a class=\"markdownIt-Anchor\" href=\"#无监督学习-unsupervised-learning\">#</a> 无监督学习. unsupervised learning</h3>\n<p>输入数据并未进行标记，没有目标值。 =&gt; 聚类 (kmeans)</p>\n<h3 id=\"半监督学习-semi-supervised-learning\"><a class=\"markdownIt-Anchor\" href=\"#半监督学习-semi-supervised-learning\">#</a> 半监督学习： semi-supervised learning</h3>\n<p>训练集同时包含有标记样本数据 和 未标记样本数据</p>\n<h3 id=\"强化学习-reinforcement-learning\"><a class=\"markdownIt-Anchor\" href=\"#强化学习-reinforcement-learning\">#</a> 强化学习: reinforcement learning</h3>\n<p>本质是 make decisions 问题，即自动决策问题，并且可以做连续决策。</p>\n<p>没有训练数据，建立模型的时候人为设定好可以操作的规则，不断自我尝试，自己去探索。</p>\n<p>强化学习的目标就是获取更多的累计奖励。</p>\n<p>举个例子:</p>\n<p>小孩子想要走路，但是在这之前，他需要先站起来，站起来之后还要保持平衡，接下来就要先迈出一条腿，是左腿还是右腿，迈出一步还要迈出下一步。</p>\n<p>小孩子就是 agent，他试图通过 行动 (即行走) 来模型环境 (行走的表面) 并且从一个状态转变到另一个状态 (即他走的每一步)，当他完成了任务的子任务（即走了几步) 时，孩子收到奖励，并且当他不能走路时，就不会给奖励。</p>\n<p>主要包含四个元素: agent  行动 环境  奖励</p>\n<p><img data-src=\"./images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB.png\" alt=\"\"></p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>期望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/rocketmq/qrcode.jpg\" alt=\"\"></p>\n",
            "tags": [
                "MachineLearn"
            ]
        }
    ]
}