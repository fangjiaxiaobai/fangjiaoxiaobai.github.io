{
    "version": "https://jsonfeed.org/version/1",
    "title": "方家小白 • All posts by \"machinelearn\" tag",
    "description": "和你一起遇见更好的自己",
    "home_page_url": "https://fangjiaxiaobai.github.io",
    "items": [
        {
            "id": "https://fangjiaxiaobai.github.io/2021/11/17/machine-learn/Model-evaluation/02-confusion_matrix/",
            "url": "https://fangjiaxiaobai.github.io/2021/11/17/machine-learn/Model-evaluation/02-confusion_matrix/",
            "title": "模型评估之 分类模型的评估指标",
            "date_published": "2021-11-17T10:18:00.000Z",
            "content_html": "<p>混淆矩阵 ( <code>Confusion Matrix</code> ) 是分类模型的一种评估指标。它是使用一种特定的矩阵来呈现算法性能的可视化效果，通常是监督学习。(非监督学习，通常用匹配矩阵：m <code>atching matrix</code> ) 其每一列代表预测值，每一行代表的是实际的类别。这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个 <code>class</code>  被预测成另一个 <code>class</code> ）。</p>\n<h2 id=\"举例\"><a class=\"markdownIt-Anchor\" href=\"#举例\">#</a> 举例</h2>\n<p>我们以 是否感染新冠肺炎的模型来演示 混淆矩阵。</p>\n<p>按照定义，我们可以得到如下矩阵。</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-1.png\" alt=\"\"></p>\n<p>横向看上图，预测感染新冠为  <code>Positive</code> , 即为阳性， 未感染新冠为  <code>Negative</code>  , 即 阴性。</p>\n<p>纵向看上图，预测正确的为  <code>True</code> ,  预测错误的为 <code>False</code> .</p>\n<p>所以会得出 如下四种结果；</p>\n<ul>\n<li><code>True Positive</code> :  <code>TP</code> , 真阳性，即预测结果是新冠，真实值也是新冠。</li>\n<li><code>True Negative</code> :  <code>TN</code> , 真阴性，即预测结果是未感染，真实也未感染新冠。</li>\n</ul>\n<p>这两个结果是模型预测正确的结果，也是我们最想得到的部分，这两部分所占比重越大，模型效果越好。对应上图中绿色部分。其占比成为模型的准确率。</p>\n<ul>\n<li><code>False Positive</code> :  <code>FP</code> , 假阳性，即预测得了新冠，但是实际人并没有感染新冠。这种的属于误报。其占比属于误报率。在统计学中，  <code>FP</code>  又称为 第一类错误 ( <code>Type Error I</code> ，被判定为真的假)</li>\n<li><code>False Negative</code> :  <code>FN</code> , 假阴性，即预测没有感染新冠，但是实例已经感染了新冠。这种属于漏报率。其占比成为漏报率。 <code>FN</code>  又称为 第二类错误 ( <code>Type Error II</code> , 被判定为假的真)</li>\n</ul>\n<p>这两种情况，是模型预测错误的场景，所占比重越低越好。对应上图中红色部分。</p>\n<h2 id=\"指标\"><a class=\"markdownIt-Anchor\" href=\"#指标\">#</a> 指标</h2>\n<p>很多指标都是从混淆矩阵发展出来的。在例子中，我们已经知道了准确率，误报率，漏报率。</p>\n<p>以图为例，我们来详细的看下分类模型中混淆矩阵的相关指标.</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-2.png\" alt=\"\"></p>\n<h3 id=\"准确率\"><a class=\"markdownIt-Anchor\" href=\"#准确率\">#</a> 🔥🔥准确率</h3>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>a</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">accuracy=\\frac{TP+TN}{TP+TN+FP+FN}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1296600000000003em;vertical-align:-0.7693300000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.36033em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693300000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>那么，本例子中的准确率，就是： <code>(10+30)/100=40%</code></p>\n<p>理解成本最低，但不要滥用。在样本不均衡情况下，指标结果容易出现较大偏差；</p>\n<h3 id=\"精确率\"><a class=\"markdownIt-Anchor\" href=\"#精确率\">#</a> 🔥🔥精确率</h3>\n<p>精确率是判断模型识别出来的结果有多精确的指标。对应到信用评分的产品上，就是模型找到的真的坏人（对应混淆矩阵中的）的比率占模型找到的所有坏人（对应混淆矩阵中的 ）的比率。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">precision=\\frac{TP}{TP+FP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1296600000000003em;vertical-align:-0.7693300000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.36033em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693300000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>用于关注筛选结果是不是正确的场景，宁可没有预测出来，也不能预测错了。比如，在刷脸支付的场景下，我们宁可告诉用户检测不通过，也不能让另外一个人的人脸通过检测。</p>\n<p>精确率，又称为 查准率.</p>\n<h3 id=\"漏报率fnr\"><a class=\"markdownIt-Anchor\" href=\"#漏报率fnr\">#</a> 漏报率 (FNR)</h3>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>F</mi><mi>N</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>N</mi></mrow><mrow><mi>F</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">FNR = \\frac{FN}{FN + TP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1296600000000003em;vertical-align:-0.7693300000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.36033em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693300000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>那么，本例子中的漏报率，就是： <code>20/(10+20)=66.67%</code></p>\n<h3 id=\"误报率\"><a class=\"markdownIt-Anchor\" href=\"#误报率\">#</a> 误报率</h3>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">FPR = \\frac{FP}{FP + TP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1296600000000003em;vertical-align:-0.7693300000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.36033em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693300000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>那么，本例子中的漏报率，就是： <code>40/(40+30)=37.14%</code></p>\n<h3 id=\"召回率\"><a class=\"markdownIt-Anchor\" href=\"#召回率\">#</a> 🔥🔥召回率</h3>\n<p>表示实际患者中，预测患病成功的概率.  <code>Recall Rate</code>  又称为  <code>Sensitive</code> , 查全率。预测感染占实际感染的额比率。即预测为真占实际为真的比率。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>S</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mi>P</mi><mi>R</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">Sensitive(TPR)=\\frac{TP}{TP+FN}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathnormal\">e</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1296600000000003em;vertical-align:-0.7693300000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.36033em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693300000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>用于关注筛选结果是不是全面的场景，“宁可错杀一千，绝不放过一个”。</p>\n<h3 id=\"specificity\"><a class=\"markdownIt-Anchor\" href=\"#specificity\">#</a> Specificity</h3>\n<p>表示未患病中，预测未患病成功的概率。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>f</mi><mi>i</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mi>N</mi><mi>R</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">Specificity(TNR)=\\frac{TN}{TN+FP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1296600000000003em;vertical-align:-0.7693300000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.36033em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693300000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>这两个指标的出现，能更好的帮你比较模型间的差异，并在其中做出取舍。例如当两个模型的  <code>Accuracy</code>  相近时，如果你更看重于预测患病的效果，你应该选  <code>Sensitivity</code>  值较高的；相反，如果你更看重于预测未患病的效果，你就应该选择  <code>Specificity</code>  较高的。</p>\n<h3 id=\"f1-score\"><a class=\"markdownIt-Anchor\" href=\"#f1-score\">#</a> 🔥🔥F1 Score</h3>\n<p><code>F1</code>  可以综合反应精确率 和召回率。  <code>F1</code>  值越高，代表模型在精确率 和 召回率的综合表现越好。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">F1=\\frac{2 \\times preciosion \\times recall}{precision+recall}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714399999999998em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804400000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h3 id=\"p-r曲线\"><a class=\"markdownIt-Anchor\" href=\"#p-r曲线\">#</a> P-R 曲线</h3>\n<p TP+FP=\"\"><code>P</code>  表示查准率 (或者精确率), 计算公式是: \\frac{TP}</p>\n<p TP+FN=\"\"><code>R</code>  表示查全率 (或者召回率，Sensitive), 计算公式是: \\frac{TP}</p>\n<p><code>P-R</code>  曲线是描述查准率 / 查全率变化的曲线， <code>P-R</code>  曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是 “正例” 的样本排在前面，最不可能是 “正例” 的排在后面，按此顺序逐个把样本作为 “正例” 进行预测，每次计算出当前的 <code>P</code>  值和 <code>R</code>  值，如下图所示：</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-4.png\" alt=\"\"></p>\n<h4 id=\"如何看p-r曲线\"><a class=\"markdownIt-Anchor\" href=\"#如何看p-r曲线\">#</a> 如何看 P-R 曲线？</h4>\n<p><code>P-R</code>  曲线如何评估呢？若一个 <code>学习器A</code>  的 <code>P-R</code>  曲线被另一个 <code>学习器B</code>  的 <code>P-R</code>  曲线完全包住，则称： <code>B</code>  的性能优于 <code>A</code> 。若 <code>A</code>  和 <code>B</code>  的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了 “平衡点”（ <code>Break-Event Point</code> ，简称 <code>BEP</code> ），即当 <code>P=R</code>  时的取值，平衡点的取值越高，性能更优。</p>\n<p>P 和 R 指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是 F-Measure，又称 F-Score。F-Measure 是 P 和 R 的加权调和平均，即：</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-5.png\" alt=\"\"></p>\n<p>特别地，当 β=1 时，也就是常见的 F1 度量，是 P 和 R 的调和平均，当 F1 较高时，模型的性能越好。</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-6.png\" alt=\"\"></p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-7.png\" alt=\"\"></p>\n<p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的 <code>P</code>  值和 <code>R</code>  值，然后取得平均 <code>P</code>  值 <code>macro-P</code>  和平均 <code>R</code>  值 <code>macro-R</code> ，在算出 <code>Fβ</code>  或 <code>F1</code> ，而微观则是计算出混淆矩阵的平均 <code>TP</code> 、 <code>FP</code> 、 <code>TN</code> 、 <code>FN</code> ，接着进行计算 <code>P</code> 、 <code>R</code> ，进而求出 <code>Fβ</code>  或 <code>F1</code> 。</p>\n<h3 id=\"roc-曲线-和-auc\"><a class=\"markdownIt-Anchor\" href=\"#roc-曲线-和-auc\">#</a> 🔥🔥ROC 曲线 和 AUC</h3>\n<p><code>ROC</code>  曲线与 <code>P-R</code>  曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是 <code>ROC</code>  曲线以 “真正例率”（ <code>True Positive Rate</code> ，简称 <code>TPR</code> , 计算公式: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{TP}{TP+FN}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.275662em;vertical-align:-0.403331em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>）为横轴，纵轴为 “假正例率”（ <code>False Positive Rate</code> ，简称 <code>FPR</code> , 计算公式: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{FP}{TN+FP}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.275662em;vertical-align:-0.403331em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>, 又称为特异性。 <code>Specificity</code> 。）， <code>ROC</code>  偏重研究基于测试样本评估值的排序好坏。</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-9.png\" alt=\"\"></p>\n<p>简单分析图像，可以得知：当 <code>FN=0</code>  时， <code>TN</code>  也必须 <code>0</code> ，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状， <code>(0,0)</code>  表示将所有的样本预测为负例， <code>(1,1)</code>  则表示将所有的样本预测为正例， <code>(0,1)</code>  表示正例全部出现在负例之前的理想情况， <code>(1,0)</code>  则表示负例全部出现在正例之前的最差情况。</p>\n<h4 id=\"怎么看-roc曲线呢\"><a class=\"markdownIt-Anchor\" href=\"#怎么看-roc曲线呢\">#</a> 怎么看 ROC 曲线呢？</h4>\n<p>若一个学习器 <code>A</code>  的 <code>ROC</code>  曲线被另一个学习器 <code>B</code>  的 <code>ROC</code>  曲线完全包住，则称 <code>B</code>  的性能优于 <code>A</code> 。若 <code>A</code>  和 <code>B</code>  的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。 <code>ROC</code>  曲线下的面积定义为 <code>AUC</code> （ <code>Area Uder ROC Curve</code> ），不同于 <code>P-R</code>  的是，这里的 <code>AUC</code>  是可估算的，即 <code>AOC</code>  曲线下每一个小矩形的面积之和。易知： <code>AUC</code>  越大，证明排序的质量越好， <code>AUC</code>  为 <code>1</code>  时，证明所有正例排在了负例的前面， <code>AUC</code>  为 <code>0</code>  时，所有的负例排在了正例的前面。</p>\n<p><code>AUC</code>  计算公式:</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-10.png\" alt=\"\"></p>\n<h3 id=\"代价敏感错误率与代价曲线\"><a class=\"markdownIt-Anchor\" href=\"#代价敏感错误率与代价曲线\">#</a> 代价敏感错误率与代价曲线</h3>\n<p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt; 有疾病只是增多了检查，但有疾病–&gt; 无疾病却是增加了生命危险。以二分类为例，由此引入了 “代价矩阵”（ <code>cost matrix</code> ）。</p>\n<p>在非均等错误代价下，我们希望的是最小化 “总体代价”，这样 “代价敏感” 的错误率为:</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-11.png\" alt=\"\"></p>\n<p>同样对于 ROC 曲线，在非均等错误代价下，演变成了 “代价曲线”，代价曲线横轴是取值在 [0,1] 之间的正例概率代价，式中 p 表示正例的概率，纵轴是取值为 [0,1] 的归一化代价。</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-12.png\" alt=\"\"><br>\n<img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-13.png\" alt=\"\"></p>\n<p>代价曲线的绘制很简单：设 <code>ROC</code>  曲线上一点的坐标为 ( <code>TPR</code> ， <code>FPR</code> ) ，则可相应计算出 <code>FNR</code> ，然后在代价平面上绘制一条从 ( <code>0</code> ， <code>FPR</code> ) 到 ( <code>1</code> ， <code>FNR</code> ) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将 <code>ROC</code>  曲线图的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-confusionMetrix-14.png\" alt=\"\"></p>\n<p>以上就是 由混淆矩阵引申出来的指标了，当然这些指标都是用来评估分类模型的。下篇文章，我们再来介绍回归模型的评估指标。</p>\n<h3 id=\"指标图\"><a class=\"markdownIt-Anchor\" href=\"#指标图\">#</a> 指标图</h3>\n<p><img data-src=\"/images/ml/model_evalution/model-evalution-4.png\" alt=\"\"></p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>希望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/qrcode.jpg\" alt=\"qrcode\"></p>\n",
            "tags": [
                "MachineLearn",
                "模型评估"
            ]
        },
        {
            "id": "https://fangjiaxiaobai.github.io/2021/11/17/machine-learn/Model-evaluation/01-overview/",
            "url": "https://fangjiaxiaobai.github.io/2021/11/17/machine-learn/Model-evaluation/01-overview/",
            "title": "模型评估概述",
            "date_published": "2021-11-17T10:18:00.000Z",
            "content_html": "<h2 id=\"分类\"><a class=\"markdownIt-Anchor\" href=\"#分类\">#</a> 分类</h2>\n<p>模型评估可以分为 离线评估和在线评估 两大类。在产品的不同阶段，我们要根据不同的场景去应用他们。</p>\n<p>两种评估方式由于其场景不同，所评估的关注点也不尽相同。其中，离线评估关注的是模型效果相关的指标，如精准率、 <code>KS</code>  等等。在线评估关注的是业务相关指标，比如新用户的转化率、优惠券的核销率、信贷审核的通过率等等。由于模型的在线评估与业务场景强相关，所以我们的课程重点将放在模型的离线评估上。</p>\n<h3 id=\"在线评估\"><a class=\"markdownIt-Anchor\" href=\"#在线评估\">#</a> 在线评估</h3>\n<p>在线评估是指在模型部署上线后，使用线上真实数据对模型进行的评估。这个时候，产品经理或者运营同学大多采用  <code>ABTest</code>  的方式去判断业务的表现.</p>\n<h3 id=\"离线评估\"><a class=\"markdownIt-Anchor\" href=\"#离线评估\">#</a> 离线评估</h3>\n<p>离线评估是指在模型部署上线前对模型进行的验证和评估工作，这个时候如果发现问题，我们可以很方便地对模型的参数进行调整和改进。</p>\n<p>离线评估又可以分为 <code>特征评估</code> 和 <code>模型评估</code> 两大类。</p>\n<h4 id=\"特征评估\"><a class=\"markdownIt-Anchor\" href=\"#特征评估\">#</a> 特征评估</h4>\n<p>为什么要关注特征评估呢？如果只评估最终模型的指标是否合规的时候，就相当于把模型作为一个 黑盒子了。但同时也要了解模型里面的内容，所以模型特征的评估也是非常重要的。 那特征评估主要关注那些内容呢？</p>\n<h5 id=\"特征自身的稳定性\"><a class=\"markdownIt-Anchor\" href=\"#特征自身的稳定性\">#</a> 特征自身的稳定性</h5>\n<p>对于特征自身的稳定，我们一般使用 <code>PSI</code>  这个指标来判断。  <code>PSI</code>  是评估某个特征的数据随着时间推移发生变化而不再稳定的指标。简单来说，就是看这个特征是不是稳定的，如果一个重要特征不够稳定，就会直接影响到模型整体的稳定性，自然也会影响业务。</p>\n<blockquote>\n<p><code>PSI</code> : ( <code>Population Stability Index</code> .  <code>PSI</code> ), 这里简单介绍一下，后面我会在一篇文章中，详细的介绍 群体稳定性 ( <code>PSI</code> ) 这个概念。 <code>PSI</code>  可用来衡量测试样本及模型开发样本评分的分布差异，为最常见的模型稳定度评估指标。计算公式为:  <code>PSI = sum(（实际占比-预期占比）* ln(实际占比/预期占比))</code> <br>\n 一般以训练集（ <code>INS</code> ）的样本分布作为预期分布，进而跨时间窗按月 / 周来计算 <code>PSI</code> ，得到 <code>Monthly/weekly PSI Report</code> ，进而剔除不稳定的变量。同理，在模型上线部署后，也将通过 <code>PSI</code>  曲线报表来观察模型的稳定性。</p>\n</blockquote>\n<h5 id=\"特征来源的稳定性\"><a class=\"markdownIt-Anchor\" href=\"#特征来源的稳定性\">#</a> 特征来源的稳定性</h5>\n<p>关于 特征来源的稳定性 评估，大致可以分为两种情况:</p>\n<ul>\n<li>如果特征数据来源于集团内部，主要考虑具体来自哪条业务线，这条业务是否稳定，以及业务方是否可能收回或者停止共享数据。</li>\n<li>如果特征接入方是外部公司，特别注意要看这个公司是否合规，是否具备完善的技术储备等等。</li>\n</ul>\n<h5 id=\"成本\"><a class=\"markdownIt-Anchor\" href=\"#成本\">#</a> 成本</h5>\n<p>在获取数据的时候，也要考虑接入的成本问题。</p>\n<ul>\n<li>公司内部数据，一般来说，不存在成本。在不同业务线的角度来说，可能会存在费用分摊的问题。</li>\n<li>外部数据，肯定是有成本的，或许是公司合作，或许是公司直接购买， 正常支付公司费用就好了。特别是注意，如果数据是按调用次数，流量计费的话，是否可以通过预先拉取数据来减少调用。</li>\n</ul>\n<h4 id=\"模型评估\"><a class=\"markdownIt-Anchor\" href=\"#模型评估\">#</a> 模型评估</h4>\n<p>模型的评估主要包括三个部分：统计性、模型性能和模型稳定性。</p>\n<h5 id=\"统计性指标\"><a class=\"markdownIt-Anchor\" href=\"#统计性指标\">#</a> 统计性指标</h5>\n<p>统计性指标指的就是模型输出结果的覆盖度、最大值、最小值、人群分布等指标。我们拿到一个模型，最先看的不是性能指标也不是稳定性，而是统计性指标，它决定了模型到底能不能用。</p>\n<p>在不同的场景下，由于我们的业务不同，对模型的要求不同，对模型统计性指标的关注点也会不同。 对统计性指标进行评估的时候，我们要充分考虑业务场景。</p>\n<p>比如:</p>\n<ul>\n<li>覆盖度。 在金融风控的场景下，如果一个模型的覆盖率低于  <code>60%</code> , 基本上就很难给到客户使用了，因为覆盖低低，风控的业务人员基本没办法对这个模型应用到决策引擎中。如果非要调用的话，最好的情况也就是用到决策树的某个分支上，专门用于某一小部分人群中，不过意义不大。</li>\n<li>最大最小值，也就是分数范围，以信用评分模型为例，如果信用评分模型覆盖的人数很多，但是模型输出的信用分数范围却很窄，假设是 <code>90-95</code> ，很显然，人群并没有好的区分度。 可以参考下芝麻分的范围就设置到了 <code>350-950</code> 。</li>\n<li>人群分布：指的是模型对人打分后，分数和人群的分布形态，这个分布形态应该符合我们的常识，比如用户消费能力评估模型，对于人群的打分结果就应该符合正态分布。</li>\n</ul>\n<h5 id=\"模型性能\"><a class=\"markdownIt-Anchor\" href=\"#模型性能\">#</a> 模型性能</h5>\n<p>模型的性能评估指标是评估模型效果的指标，他和模型要解决的问题相关， 模型要解决的问题，可以分成分类问题和回归问题。</p>\n<h6 id=\"分类模型\"><a class=\"markdownIt-Anchor\" href=\"#分类模型\">#</a> 分类模型</h6>\n<p>分类模型的性能评价指标主要包括：  <code>混淆矩阵</code> ， <code>KS</code> ，  <code>AUC</code>  等等。分类模型的性能评价指标主要包括： <code>混淆矩阵</code> 、 <code>KS</code> 、 <code>AUC</code>  等等。通过混淆矩阵，我们既可以得到一个模型的精确率、召回率这些指标，从而可以评估一个模型的区分能力，我们也可以计算得到的  <code>TPR</code> 、 <code>FPR</code> ，从而计算出  <code>AUC</code> 、 <code>KS</code>  等相关指标。因此，混淆矩阵是评估二分类模型的基础工具。</p>\n<h6 id=\"回归模型\"><a class=\"markdownIt-Anchor\" href=\"#回归模型\">#</a> 回归模型</h6>\n<p>回归模型的性能评价指标主要包括  <code>MAE</code>  (平均绝对误差),  <code>MSE</code>  (均方误差),  <code>RMSE</code>  (均方根误差), <code>R方</code> 。</p>\n<h5 id=\"模型稳定性\"><a class=\"markdownIt-Anchor\" href=\"#模型稳定性\">#</a> 模型稳定性</h5>\n<p>模型的稳定性即判断模型输出结果，是否会随着时间推移，而发生较大变化不再稳定的指标，模型的稳定性会直接影响模型的结果。比如在风控场景下，如果风控模型不够稳定，对于用户风险判断的结果就会发生较大变化。这个时候，我们需要实时调整风控策略，同时也要注意调整后造成决策不合理的情况。对于模型的稳定性，我们主要使用  <code>PSI</code>  进行评估。</p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>希望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/qrcode.jpg\" alt=\"qrcode\"></p>\n",
            "tags": [
                "MachineLearn",
                "模型评估"
            ]
        },
        {
            "id": "https://fangjiaxiaobai.github.io/2021/10/29/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A002-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/",
            "url": "https://fangjiaxiaobai.github.io/2021/10/29/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A002-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/",
            "title": "K-近邻算法(KNN)",
            "date_published": "2021-10-29T10:18:00.000Z",
            "content_html": "<p><code>k-近邻算法</code> ，英文名:  <code>K Nearest Neighbor</code>  算法  又叫 <code>KNN算法</code> ，这个算法是机器学习里面一个比较经典的算法， 总体来说 1 算法是相对比较容易理解的算法.</p>\n<h2 id=\"定义\"><a class=\"markdownIt-Anchor\" href=\"#定义\">#</a> 定义</h2>\n<p>如果一个样本在特征空间中的 k 个最相似 (即特征空间中最邻近) 的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>\n<blockquote>\n<p><code>KNN</code>  算法最早是由 <code>Cover</code>  和 <code>Hart</code>  提出的一种分类算法。应用场景有字符识别、文本分类、图像识别等领域。</p>\n</blockquote>\n<h2 id=\"算法的理解\"><a class=\"markdownIt-Anchor\" href=\"#算法的理解\">#</a> 算法的理解</h2>\n<p>举一个例子来，来分析一下  <code>KNN</code>  算法的实现原理</p>\n<p>假设我们现在有几部电影，如下图:</p>\n<p><img data-src=\"/images/ml/02-knn-1.png\" alt=\"\"></p>\n<p>我们要 根据 搞笑镜头，拥抱镜头，打斗镜头的个数 这三个特征来预测出 《唐人街探案》所属的电影类型.</p>\n<p>我们使用  <code>KNN算法</code>  思想来实现预测。</p>\n<p>将样本中特征作为坐标抽，建立坐标系。从而建立特征空间。本例中，分别把 搞笑镜头，拥抱镜头，打斗镜头 作为 <code>x,y,z</code>  轴。然后把计算出每个样本和 《唐人街探案》 的距离，选择距离最近的前 k 个 ( <code>KNN中的k</code> ) 样本，这 <code>k</code>  个样本大多数所属的电影类别就是 《唐人街探案》的电影类型。</p>\n<blockquote>\n<p>特征空间：是指已特征为坐标轴简历的一种特征的坐标系，可能是多维的。</p>\n</blockquote>\n<p>那你可能对 距离 是如何计算的，有点疑惑。计算距离的方式有很多种。<br>\n先学习一下最简单的 欧氏距离。(初中都学过的！)</p>\n<p>在二维坐标系中， 我们可以使用一下方式来计算出两个点的距离。</p>\n<p><img data-src=\"/images/ml/02-knn-2.png\" alt=\"\"></p>\n<p>在多维的特征空间中，我们也可以使用同样的方式来计算欧式距离。如下图：</p>\n<p><img data-src=\"/images/ml/02-knn-3.png\" alt=\"\"></p>\n<p>那计算一下样本集中的欧式距离，如下图:</p>\n<p><img data-src=\"/images/ml/02-knn-4.png\" alt=\"\"></p>\n<p>并计算出了最近的 5 个样本中有三个喜剧片类型，2 两个爱情片类型。 那根据 <code>KNN</code>  就是喜剧片类型。</p>\n<p>这就是 <code>KNN算法</code> 的核心思想了。</p>\n<p>这个例子中，我们选则了 最近的 <code>5</code>  个样本，也就是 k=5 的时候，会有三个喜剧片类型，两个爱情片类型。这个 <code>5</code>  是如何选择的呢？</p>\n<h2 id=\"k值的选择\"><a class=\"markdownIt-Anchor\" href=\"#k值的选择\">#</a> k 值的选择</h2>\n<p><code>K值</code> 选择问题，李航博士的一书「统计学习方法」上所说：</p>\n<ol>\n<li>\n<p>选择较小的 <code>K</code>  值，就相当于用较小的领域中的训练实例进行预测，“学习” 近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是 “学习” 的估计误差会增大，换句话说， <code>K值</code> 的减小就意味着整体模型变得复杂，容易发生过拟合；</p>\n</li>\n<li>\n<p>选择较大的 <code>K值</code> ，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且 K 值的增大就意味着整体的模型变得简单。</p>\n</li>\n<li>\n<p><code>K=N</code> （ <code>N</code>  为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。</p>\n</li>\n</ol>\n<p>在实际应用中， <code>K值</code> 一般取一个比较小的数值，例如采用交叉验证法 ( <code>cross validation</code> )（简单来说，就是把训练数据在分成两组：训练集和验证集）来选择最优的 K 值。对这个简单的分类器进行泛化，用核方法把这个线性模型扩展到非线性的情况，具体方法是把低维数据集映射到高维特征空间。</p>\n<h2 id=\"knn的优化\"><a class=\"markdownIt-Anchor\" href=\"#knn的优化\">#</a> KNN 的优化</h2>\n<p><code>KNN</code>  算法需要计算所有的样本数据和预测数据的距离，需要选择出距离预测数据最近的 k 个样本数据的预测归类。在庞大的数据量面前，计算所有样本数据距离，显然是不可取的。为了避免每次都重新计算一遍距离， <code>KNN</code>  算法提供了多种优化方法， 比如  <code>KD-tree</code> ,  <code>ball_tree</code> ,  <code>brute</code> . 这几种优化方式的具体实现逻辑，我会在后面的几篇文章中挨个分析。</p>\n<h2 id=\"距离的计算\"><a class=\"markdownIt-Anchor\" href=\"#距离的计算\">#</a> 距离的计算</h2>\n<p><code>KNN</code>  算法，最重要的就是距离。 除了上文提到的欧式距离，还有其他计算距离的方法吗？</p>\n<p>有。</p>\n<p>除了欧式距离，还有 曼哈顿距离，</p>\n<h3 id=\"曼哈顿距离\"><a class=\"markdownIt-Anchor\" href=\"#曼哈顿距离\">#</a> 曼哈顿距离</h3>\n<p>在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是 “曼哈顿距离”。曼哈顿距离也称为 “城市街区距离”( <code>City Block distance</code> )。如下图:<br>\n<img data-src=\"/images/ml/02-knn-6.png\" alt=\"\"></p>\n<p>计算公式见下图:</p>\n<p><img data-src=\"/images/ml/02-knn-7.png\" alt=\"\"></p>\n<h3 id=\"切比雪夫距离-chebyshev-distance\"><a class=\"markdownIt-Anchor\" href=\"#切比雪夫距离-chebyshev-distance\">#</a> 切比雪夫距离 (Chebyshev Distance)</h3>\n<p>国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻 8 个方格中的任意一个。国王从格子 <code>(x1,y1)</code>  走到格子 <code>(x2,y2)</code>  最少需要多少步？这个距离就叫切比雪夫距离。</p>\n<p><img data-src=\"/images/ml/02-knn-8.png\" alt=\"\"></p>\n<p>计算公式见下图:</p>\n<p><img data-src=\"/images/ml/02-knn-9.png\" alt=\"\"></p>\n<h3 id=\"闵可夫斯基距离minkowski-distance\"><a class=\"markdownIt-Anchor\" href=\"#闵可夫斯基距离minkowski-distance\">#</a> 闵可夫斯基距离 (Minkowski Distance)</h3>\n<p>闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。</p>\n<p>两个 n 维变量 <code>a(x11,x12,…,x1n)</code>  与 <code>b(x21,x22,…,x2n)</code>  间的闵可夫斯基距离定义为：</p>\n<p><img data-src=\"/images/ml/02-knn-10.png\" alt=\"\"></p>\n<p>其中 <code>p</code>  是一个变参数：<br>\n当 <code>p=1</code>  时，就是曼哈顿距离；<br>\n当 <code>p=2</code>  时，就是欧氏距离；<br>\n当 <code>p→∞</code> 时，就是切比雪夫距离。</p>\n<p>根据 p 的不同，闵氏距离可以表示某一类 / 种的距离。</p>\n<p>小结：<br>\n1 闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点:<br>\n <code>e.g.</code>  二维样本 (身高 <code>[单位:cm]</code> , 体重 <code>[单位:kg]</code> ), 现有三个样本： <code>a(180,50)</code> ， <code>b(190,50)</code> ， <code>c(180,60)</code> 。<br>\n <code>a</code>  与 <code>b</code>  的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于 <code>a</code>  与 <code>c</code>  的闵氏距离。但实际上身高的 <code>10cm</code>  并不能和体重的 <code>10kg</code>  划等号。</p>\n<p>2 闵氏距离的缺点：<br>\n​ (1) 将各个分量的量纲 ( <code>scale</code> )，也就是 “单位” 相同的看待了；<br>\n​ (2) 未考虑各个分量的分布（期望，方差等）可能是不同的。</p>\n<h3 id=\"标准化欧氏距离-standardized-euclideandistance\"><a class=\"markdownIt-Anchor\" href=\"#标准化欧氏距离-standardized-euclideandistance\">#</a> 标准化欧氏距离 (Standardized EuclideanDistance)</h3>\n<p>标准化欧氏距离是针对欧氏距离的缺点而作的一种改进。</p>\n<p>思路：既然数据各维分量的分布不一样，那先将各个分量都 “标准化” 到均值、方差相等。假设样本集 X 的均值 ( <code>mean</code> ) 为 <code>m</code> ，标准差 ( <code>standard deviation</code> ) 为 <code>s</code> ， <code>X</code>  的 “标准化变量” 表示为：</p>\n<p><img data-src=\"/images/ml/02-knn-11.png\" alt=\"\"></p>\n<p>如果将方差的倒数看成一个权重，也可称之为加权欧氏距离 ( <code>Weighted Euclidean distance</code> )。</p>\n<h3 id=\"余弦距离cosine-distance\"><a class=\"markdownIt-Anchor\" href=\"#余弦距离cosine-distance\">#</a> 余弦距离 (Cosine Distance)</h3>\n<p>几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。</p>\n<p>二维空间中向量 <code>A(x1,y1)</code>  与向量 <code>B(x2,y2)</code>  的夹角余弦公式：<br>\n<img data-src=\"/images/ml/02-knn-12.png\" alt=\"\"></p>\n<p>两个 <code>n</code>  维样本点 <code>a(x11,x12,…,x1n)</code>  和 <code>b(x21,x22,…,x2n)</code>  的夹角余弦为：</p>\n<p><img data-src=\"/images/ml/02-knn-13.png\" alt=\"\"></p>\n<p>即:</p>\n<p><img data-src=\"/images/ml/02-knn-14.png\" alt=\"\"></p>\n<p>夹角余弦取值范围为 <code>[-1,1]</code> 。余弦越大表示两个向量的夹角越小，余弦越小表示两向量的夹角越大。当两个向量的方向重合时余弦取最大值 <code>1</code> ，当两个向量的方向完全相反余弦取最小值 <code>-1</code></p>\n<h3 id=\"汉明距离hamming-distance\"><a class=\"markdownIt-Anchor\" href=\"#汉明距离hamming-distance\">#</a> 汉明距离 (Hamming Distance)</h3>\n<p>两个等长字符串 <code>s1</code>  与 <code>s2</code>  的汉明距离为：将其中一个变为另外一个所需要作的最小字符替换次数。</p>\n<p>例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The Hamming distance between &quot;1011101&quot; and &quot;1001001&quot; is 2. </span><br><span class=\"line\">The Hamming distance between &quot;2143896&quot; and &quot;2233796&quot; is 3. </span><br><span class=\"line\">The Hamming distance between &quot;toned&quot; and &quot;roses&quot; is 3.</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"/images/ml/02-knn-15.png\" alt=\"\"></p>\n<p>汉明重量：是字符串相对于同样长度的零字符串的汉明距离，也就是说，它是字符串中非零的元素个数：对于二进制字符串来说，就是  <code>1</code>  的个数，所以  <code>11101</code>  的汉明重量是  <code>4</code> 。因此，如果向量空间中的元素 <code>a</code>  和 <code>b</code>  之间的汉明距离等于它们汉明重量的差 <code>a-b</code> 。</p>\n<p>应用：汉明重量分析在包括信息论、编码理论、密码学等领域都有应用。比如在信息编码过程中，为了增强容错性，应使得编码间的最小汉明距离尽可能大。但是，如果要比较两个不同长度的字符串，不仅要进行替换，而且要进行插入与删除的运算，在这种场合下，通常使用更加复杂的编辑距离等算法。</p>\n<h3 id=\"杰卡德距离jaccard-distance\"><a class=\"markdownIt-Anchor\" href=\"#杰卡德距离jaccard-distance\">#</a> 杰卡德距离 (Jaccard Distance)</h3>\n<p>杰卡德相似系数 ( <code>Jaccard similarity coefficient</code> )：两个集合 <code>A</code>  和 <code>B</code>  的交集元素在 <code>A</code> ， <code>B</code>  的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号 <code>J(A,B</code> ) 表示：</p>\n<p><img data-src=\"/images/ml/02-knn-16.png\" alt=\"\"></p>\n<p>杰卡德距离 ( <code>Jaccard Distance</code> )：与杰卡德相似系数相反，用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度：</p>\n<p><img data-src=\"/images/ml/02-knn-17.png\" alt=\"\"></p>\n<h3 id=\"马氏距离mahalanobis-distance\"><a class=\"markdownIt-Anchor\" href=\"#马氏距离mahalanobis-distance\">#</a> 马氏距离 (Mahalanobis Distance)</h3>\n<p>下图有两个正态分布图，它们的均值分别为 <code>a</code>  和 <code>b</code> ，但方差不一样，则图中的 <code>A</code>  点离哪个总体更近？或者说 <code>A</code>  有更大的概率属于谁？显然， <code>A</code>  离左边的更近， <code>A</code>  属于左边总体的概率更大，尽管 <code>A</code>  与 <code>a</code>  的欧式距离远一些。这就是马氏距离的直观解释。</p>\n<p><img data-src=\"/images/ml/02-knn-18.png\" alt=\"\"></p>\n<p>马氏距离是基于样本分布的一种距离。</p>\n<p>马氏距离是由印度统计学家马哈拉诺比斯提出的，表示数据的协方差距离。它是一种有效的计算两个位置样本集的相似度的方法。</p>\n<p>与欧式距离不同的是，它考虑到各种特性之间的联系，即独立于测量尺度。</p>\n<p>马氏距离定义：设总体 <code>G</code>  为 <code>m</code>  维总体（考察 <code>m</code>  个指标），均值向量为 <code>μ=（μ1，μ2，… ...，μm，）</code> , 协方差阵为 <code>∑=（σij）</code> ,</p>\n<p>则样本 <code>X=（X1，X2，… …，Xm，）</code> 与总体 G 的马氏距离定义为：</p>\n<p><img data-src=\"/images/ml/02-knn-19.png\" alt=\"\"></p>\n<p>马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为 <code>∑</code> 的随机变量的差异程度：如果协方差矩阵为单位矩阵，马氏距离就简化为欧式距离；如果协方差矩阵为对角矩阵，则其也可称为正规化的欧式距离。</p>\n<h4 id=\"马氏距离特性\"><a class=\"markdownIt-Anchor\" href=\"#马氏距离特性\">#</a> 马氏距离特性：</h4>\n<p>1. 量纲无关，排除变量之间的相关性的干扰；</p>\n<p>2. 马氏距离的计算是建立在总体样本的基础上的，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；</p>\n<p>3 . 计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离计算即可。</p>\n<p>4. 还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如三个样本点 <code>(3,4)</code> ， <code>(5,6)</code> ， <code>(7,8)</code> ，这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。</p>\n<h4 id=\"欧式距离马氏距离\"><a class=\"markdownIt-Anchor\" href=\"#欧式距离马氏距离\">#</a> 欧式距离 &amp; 马氏距离：</h4>\n<p><img data-src=\"/images/ml/02-knn-20.png\" alt=\"\"></p>\n<p>举例：</p>\n<p>已知有两个类 <code>G1</code>  和 <code>G2</code> ，比如 <code>G1</code>  是设备 <code>A</code>  生产的产品， <code>G2</code>  是设备 <code>B</code>  生产的同类产品。设备 <code>A</code>  的产品质量高（如考察指标为耐磨度 <code>X</code> ），其平均耐磨度 <code>μ1=80</code> ，反映设备精度的方差 <code>σ2(1)=0.25</code> ; 设备 B 的产品质量稍差，其平均耐磨损度 <code>μ2=75</code> ，反映设备精度的方差 <code>σ2(2)=4</code> .</p>\n<p>今有一产品 <code>G0</code> ，测的耐磨损度 <code>X0=78</code> ，试判断该产品是哪一台设备生产的？</p>\n<p>直观地看， <code>X0</code>  与 <code>μ1</code> （ <code>设备A</code> ）的绝对距离近些，按距离最近的原则，是否应把该产品判断 <code>设备A</code>  生产的？</p>\n<p>考虑一种相对于分散性的距离，记 <code>X0</code>  与 <code>G1</code> ， <code>G2</code>  的相对距离为 <code>d1</code> ， <code>d2</code> , 则：</p>\n<p><img data-src=\"/images/ml/02-knn-21.png\" alt=\"\"></p>\n<p>因为 <code>d2=1.5 &lt; d1=4</code> ，按这种距离准则，应判断 <code>X0</code>  为设备 B 生产的。</p>\n<p>设备 <code>B</code>  生产的产品质量较分散，出现 <code>X0</code>  为 <code>78</code>  的可能性较大；而 <code>设备A</code>  生产的产品质量较集中，出现 <code>X0</code>  为 <code>78</code>  的可能性较小。</p>\n<p>这种相对于分散性的距离判断就是马氏距离。</p>\n<p><img data-src=\"/images/ml/02-knn-22.png\" alt=\"\"></p>\n<h2 id=\"案例\"><a class=\"markdownIt-Anchor\" href=\"#案例\">#</a> 案例</h2>\n<h3 id=\"预测鸢尾花种类\"><a class=\"markdownIt-Anchor\" href=\"#预测鸢尾花种类\">#</a> 预测鸢尾花种类</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iris_demo</span>():</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.准备数据</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程</span></span><br><span class=\"line\">    <span class=\"comment\"># 3.1 标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器训练(模型训练)</span></span><br><span class=\"line\">    estimator = KNeighborsClassifier(n_neighbors=<span class=\"number\">3</span>)</span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1  方法1：比对真实值和预测值</span></span><br><span class=\"line\">    predict_data = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为: \\n&quot;</span>, predict_data)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;比对真实值和预测值;\\n&quot;</span>, predict_data == y_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2  方法2: 直接计算正确率</span></span><br><span class=\"line\">    score = estimator.score(x_test, y_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;正确率:&quot;</span>, score)</span><br></pre></td></tr></table></figure>\n<h4 id=\"输出结果\"><a class=\"markdownIt-Anchor\" href=\"#输出结果\">#</a> 输出结果</h4>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">预测值为: </span><br><span class=\"line\"> [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2]</span><br><span class=\"line\">比对真实值和预测值;</span><br><span class=\"line\"> [ True  True  True  True  True  True  True  True  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True False  True  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True]</span><br><span class=\"line\">正确率: 0.9666666666666667</span><br></pre></td></tr></table></figure>\n<h4 id=\"使用-gscv-优化\"><a class=\"markdownIt-Anchor\" href=\"#使用-gscv-优化\">#</a> 使用 GSCV 优化</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split, GridSearchCV</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iris_demo</span>():</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.准备数据</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程</span></span><br><span class=\"line\">    <span class=\"comment\"># 3.1 标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器训练(模型训练)</span></span><br><span class=\"line\">    estimator = KNeighborsClassifier()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.1 准备要调的超参数</span></span><br><span class=\"line\">    param_dict = &#123;<span class=\"string\">&quot;n_neighbors&quot;</span>: [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>]&#125;</span><br><span class=\"line\">    <span class=\"comment\"># 4.2 创建 GridSearchCV,使用网格搜索和交叉验证</span></span><br><span class=\"line\">    estimator = GridSearchCV(estimator, param_grid=param_dict, cv=<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1  方法1：比对真实值和预测值</span></span><br><span class=\"line\">    predict_data = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为: \\n&quot;</span>, predict_data)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;比对真实值和预测值;\\n&quot;</span>, predict_data == y_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2  方法2: 直接计算正确率</span></span><br><span class=\"line\">    score = estimator.score(x_test, y_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;正确率:&quot;</span>, score)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 6. 直接查看评估结果哦</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;在交叉验证中验证的最好结果：&quot;</span>, estimator.best_score_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;最好的参数模型：&quot;</span>, estimator.best_estimator_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;每次交叉验证后的准确率结果：\\n&quot;</span>, estimator.cv_results_)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    iris_demo()</span><br></pre></td></tr></table></figure>\n<h4 id=\"输出结果-2\"><a class=\"markdownIt-Anchor\" href=\"#输出结果-2\">#</a> 输出结果</h4>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">预测值为: </span><br><span class=\"line\"> [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0</span><br><span class=\"line\"> 0]</span><br><span class=\"line\">比对真实值和预测值;</span><br><span class=\"line\"> [ True  True  True  True  True  True  True False  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True False  True  True  True  True  True</span><br><span class=\"line\">  True  True  True  True  True  True  True  True  True  True  True  True</span><br><span class=\"line\">  True  True]</span><br><span class=\"line\">正确率: 0.9473684210526315</span><br><span class=\"line\">在交叉验证中验证的最好结果： 0.9732100521574205</span><br><span class=\"line\">最好的参数模型： KNeighborsClassifier()</span><br><span class=\"line\">每次交叉验证后的准确率结果：</span><br><span class=\"line\"> &#123;&#x27;mean_fit_time&#x27;: array([0.0008928 , 0.00045244, 0.00044529]), &#x27;std_fit_time&#x27;: array([5.74547103e-04, 5.05512361e-06, 2.92218150e-06]), &#x27;mean_score_time&#x27;: array([0.00226967, 0.00184425, 0.00182239]), &#x27;std_score_time&#x27;: array([6.28895378e-04, 2.09757168e-05, 1.41269575e-05]), &#x27;param_n_neighbors&#x27;: masked_array(data=[1, 3, 5],</span><br><span class=\"line\">             mask=[False, False, False],</span><br><span class=\"line\">       fill_value=&#x27;?&#x27;,</span><br><span class=\"line\">            dtype=object), &#x27;params&#x27;: [&#123;&#x27;n_neighbors&#x27;: 1&#125;, &#123;&#x27;n_neighbors&#x27;: 3&#125;, &#123;&#x27;n_neighbors&#x27;: 5&#125;], &#x27;split0_test_score&#x27;: array([0.97368421, 0.97368421, 0.97368421]), &#x27;split1_test_score&#x27;: array([0.97297297, 0.97297297, 0.97297297]), &#x27;split2_test_score&#x27;: array([0.94594595, 0.89189189, 0.97297297]), &#x27;mean_test_score&#x27;: array([0.96420104, 0.94618303, 0.97321005]), &#x27;std_test_score&#x27;: array([0.01291157, 0.03839073, 0.00033528]), &#x27;rank_test_score&#x27;: array([2, 3, 1], dtype=int32)&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"预测facebook签到位置\"><a class=\"markdownIt-Anchor\" href=\"#预测facebook签到位置\">#</a> 预测 facebook 签到位置</h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">facebook_demo</span>():</span></span><br><span class=\"line\">    <span class=\"comment\"># 准备数据</span></span><br><span class=\"line\">    data = pd.read_csv(<span class=\"string\">&#x27;./train.csv&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(data.head())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">## 2.1 处理时间特征</span></span><br><span class=\"line\">    time = pd.to_datetime(data[<span class=\"string\">&#x27;time&#x27;</span>], unit=<span class=\"string\">&#x27;s&#x27;</span>)</span><br><span class=\"line\">    time = pd.DatetimeIndex(time)</span><br><span class=\"line\"></span><br><span class=\"line\">    data[<span class=\"string\">&#x27;hour&#x27;</span>] = time.hour</span><br><span class=\"line\">    data[<span class=\"string\">&#x27;day&#x27;</span>] = time.day</span><br><span class=\"line\">    data[<span class=\"string\">&#x27;weekday&#x27;</span>] = time.weekday</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(data.head(<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.2.去掉签到少的地方</span></span><br><span class=\"line\">    place_count = data.groupby(<span class=\"string\">&quot;place_id&quot;</span>).count()</span><br><span class=\"line\">    place_count = place_count[place_count[<span class=\"string\">&quot;row_id&quot;</span>] &gt; <span class=\"number\">3</span>]</span><br><span class=\"line\">    data = data[data[<span class=\"string\">&quot;place_id&quot;</span>].isin(place_count.index)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.3 确定特征和目标值</span></span><br><span class=\"line\">    x = data[[<span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;y&#x27;</span>, <span class=\"string\">&quot;accuracy&quot;</span>, <span class=\"string\">&quot;day&quot;</span>, <span class=\"string\">&quot;hour&quot;</span>, <span class=\"string\">&quot;weekday&quot;</span>]]</span><br><span class=\"line\">    y = data[[<span class=\"string\">&#x27;place_id&#x27;</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.4 拆分数据集</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3. 特征处理</span></span><br><span class=\"line\">    <span class=\"comment\"># 3.1 标准化处理</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器学习</span></span><br><span class=\"line\">    <span class=\"comment\"># 4.1 实例化估计器</span></span><br><span class=\"line\">    estimator = KNeighborsClassifier()</span><br><span class=\"line\">    param_dict = &#123;<span class=\"string\">&#x27;neighbors&#x27;</span>: [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>]&#125;</span><br><span class=\"line\">    estimator = GridSearchCV(estimator=estimator, param_grid=param_dict)</span><br><span class=\"line\">    <span class=\"comment\"># 4.2 模型训练</span></span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 模型评估</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n最后预测的准确率为: &quot;</span>, estimator.score(x_test, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n在交叉验证中验证的最好结果:\\n&quot;</span>, estimator.best_score_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n最好的参数模型:\\n&quot;</span>, estimator.best_estimator_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n每次交叉验证后的验证集准确率结果和训练集准确率结果:\\n&quot;</span>, estimator.cv_results_)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    facebook_demo()</span><br></pre></td></tr></table></figure>\n<p>本案例来自  <code>Kaggle</code>  的题目，感兴趣的朋友可以登录:<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9uYXZvc2h0YS9ncmlkLWtubi9zY3JpcHQ=\">https://www.kaggle.com/navoshta/grid-knn/script</span>  查看</p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>希望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/ml/qrcode.jpg\" alt=\"qrcode\"></p>\n",
            "tags": [
                "MachineLearn",
                "KNN"
            ]
        },
        {
            "id": "https://fangjiaxiaobai.github.io/2021/10/28/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A001-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BA%86%E8%A7%A3/",
            "url": "https://fangjiaxiaobai.github.io/2021/10/28/machine-learn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A001-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BA%86%E8%A7%A3/",
            "title": "机器学习-简介",
            "date_published": "2021-10-28T10:18:00.000Z",
            "content_html": "<h2 id=\"概述\"><a class=\"markdownIt-Anchor\" href=\"#概述\">#</a> 概述</h2>\n<h3 id=\"什么是机器学习\"><a class=\"markdownIt-Anchor\" href=\"#什么是机器学习\">#</a> 什么是机器学习</h3>\n<p>从历史数据中自动分析获得规律 (模型), 并利用规律对未知数据进行预测。</p>\n<h3 id=\"为什么需要机器学习\"><a class=\"markdownIt-Anchor\" href=\"#为什么需要机器学习\">#</a> 为什么需要机器学习</h3>\n<p>解放生产力：智能客服<br>\n解决专业问题: ET 医疗<br>\n提供社会便利：提供社会便利</p>\n<h3 id=\"机器学习的应用场景\"><a class=\"markdownIt-Anchor\" href=\"#机器学习的应用场景\">#</a> 机器学习的应用场景</h3>\n<p>方方面面</p>\n<h2 id=\"机器学习的工作流程\"><a class=\"markdownIt-Anchor\" href=\"#机器学习的工作流程\">#</a> 机器学习的工作流程</h2>\n<p>获取数据 -&gt; 数据基本处理 -&gt; 特征工程 -&gt; 机器学习 (模型训练) -&gt; 模型评估。</p>\n<p>评估符合要求，则上线服务。如果不符合要求，则重复上述步骤。</p>\n<h2 id=\"数据集的结构\"><a class=\"markdownIt-Anchor\" href=\"#数据集的结构\">#</a> 数据集的结构</h2>\n<p>机器学习的数据大部分数据存储到文件中。数据库中间件 (mysql,nosql) 等性能瓶颈，读取速度，格式不符合机器学习要求的数据格式。<br>\nnumpy 非常快，因为什么？释放了 GIL。</p>\n<p>在数据集中：<br>\n一行数据成为一个样本。<br>\n一列数据称为一个特征。</p>\n<h3 id=\"可用的数据集\"><a class=\"markdownIt-Anchor\" href=\"#可用的数据集\">#</a> 可用的数据集</h3>\n<p>Kaggle: 大数据竞赛平台，80 万科学家，真实数据，数据量巨大 <span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cua2FnZ2xlLmNvbS9kYXRhc2V0cw==\">https://www.kaggle.com/datasets</span></p>\n<p>UCI: 收录了 360 个数据集。覆盖科学，生活经济等领域，数据量几十万 <span class=\"exturl\" data-url=\"aHR0cDovL2FyY2hpdmUuaWNzLnVjaS5lZHUvbWwvaW5kZXgucGhw\">http://archive.ics.uci.edu/ml/index.php</span></p>\n<p>scikit-learn: 数据量小，方便学习。<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zY2lraXQtbGVhcm4ub3JnL3N0YWJsZS8saHR0cHM6Ly9zY2lraXQtbGVhcm4ub3JnL3N0YWJsZS9zdGFibGUvZGF0YXNldHMvaW5kZXguaHRtbA==\">https://scikit-learn.org/stable/,https://scikit-learn.org/stable/stable/datasets/index.html</span></p>\n<h3 id=\"常用数据集数据的结构组成\"><a class=\"markdownIt-Anchor\" href=\"#常用数据集数据的结构组成\">#</a> 常用数据集数据的结构组成</h3>\n<ul>\n<li>特征值 + 目标值 (目标值是练习的和离散的)。特征？比如分辨男女，一个人身高体重皮肤颜色，头发长度。都是特征值。目标值：这个人是男是女，就是目标值。</li>\n<li>只有特征值，没有目标值</li>\n</ul>\n<h3 id=\"数据分割\"><a class=\"markdownIt-Anchor\" href=\"#数据分割\">#</a> 数据分割</h3>\n<p>机器学习一般的数据集会划分为两个部分。</p>\n<ul>\n<li>训练数据：用于训练，构建模型</li>\n<li>测试数据：在数据检验时使用，用于评估模型是否有效、</li>\n</ul>\n<p>划分比例：</p>\n<ul>\n<li>训练集： 79% 80% 75%</li>\n<li>测试集： 30%， 20% 25%</li>\n</ul>\n<h2 id=\"数据的特征工程\"><a class=\"markdownIt-Anchor\" href=\"#数据的特征工程\">#</a> 数据的特征工程</h2>\n<h3 id=\"数据特征是什么\"><a class=\"markdownIt-Anchor\" href=\"#数据特征是什么\">#</a> 数据特征是什么</h3>\n<p>将原始数据转换为更好的代表预测模型的潜在问题的特征的过程，从而提高对未知数据的预测准确性。</p>\n<p>使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发送更好的作用的过程。</p>\n<ul>\n<li>\n<p>特征提取： 将任意数据 (文本或者图像) 转换为可用于机器学习的数字特征。</p>\n</li>\n<li>\n<p>特征预处理：通过转化函数将特征数据转换成更加适合算法模型的特征数据过程。</p>\n<ul>\n<li>归一化：为什么要进行归一化？特征的单位或者大小相差较大，或者某特征的方法相比其他特征要大出几个数量级，容易影响 (支配) 目标结果，使得算法无法学习到其他的特征。\n<ul>\n<li>归一化的定义： 通过对原始数据进行变换，把数据映射到默认 <code>[0,1]</code>  之间。<br>\n计算公式:<br>\n<img data-src=\"./images/%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>标准化：通过对原始数据进行把数据变换到均值为 0，标准差为 1 的范围内。<br>\n<img data-src=\"./images/%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"\"><br>\n对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变<br>\n对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。</li>\n</ul>\n</li>\n<li>\n<p>特征降维：指在限定条件下，降低随机变量 (特征) 个数，得到一组 &quot;不相关&quot; 主变量的过程。<br>\n减少特征的数量，同时保留原来数据的大部分信息。可以通过 PCA 算法来是实现特征降维。</p>\n</li>\n</ul>\n<p>为什么要降维呢？随着数据维度不断降低，数据存储所需的空间也会随之减少。低维数据有助于减少计算 / 训练用时。一些算法在高维度数据上容易表现不佳，降维可提高算法可用性。降维可以用删除冗余特征解决多重共线性问题。比如我们有两个变量：“一段时间内在跑步机上的耗时” 和 “卡路里消耗量”。这两个变量高度相关，在跑步机上花的时间越长，燃烧的卡路里自然就越多。因此，同时存储这两个数据意义不大，只需一个就够了。降维有助于数据可视化。如前所述，如果数据维度很高，可视化会变得相当困难，而绘制二维三维数据的图表非常简单。</p>\n<h3 id=\"特征工程的意义\"><a class=\"markdownIt-Anchor\" href=\"#特征工程的意义\">#</a> 特征工程的意义</h3>\n<p>直接影响预测结果。</p>\n<h4 id=\"特征抽取实例演示\"><a class=\"markdownIt-Anchor\" href=\"#特征抽取实例演示\">#</a> 特征抽取实例演示</h4>\n<h4 id=\"特征抽取api\"><a class=\"markdownIt-Anchor\" href=\"#特征抽取api\">#</a> 特征抽取 API</h4>\n<p>字典特征抽取类:   <code>sklearn.feature_extraction.DictVectorizer</code> <br>\n 作用：对字典进行特征值化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DictVectorizer(sparse=<span class=\"literal\">True</span>,....)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># X: 字典或者包含字典的迭代器</span></span><br><span class=\"line\"><span class=\"comment\"># 返回值: 返回sparse矩阵</span></span><br><span class=\"line\">DictVectorizer.fit_transform(X):</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># X: array数组或者sparse矩阵</span></span><br><span class=\"line\"><span class=\"comment\"># 返回值: 转换之前的数据格式</span></span><br><span class=\"line\">DictVectorizer.invers_transform(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 返回类别名称</span></span><br><span class=\"line\">DictVectorizer.get_feature_names()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 按照原先的标准转换</span></span><br><span class=\"line\">DictVectorizerr.transform(X)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h5 id=\"使用流程\"><a class=\"markdownIt-Anchor\" href=\"#使用流程\">#</a> 使用流程</h5>\n<p>1. 实例化类 DictVectorizer<br>\n2. 调用 fit_transform 方法进行特征抽取</p>\n<h4 id=\"字典数据提取的现象\"><a class=\"markdownIt-Anchor\" href=\"#字典数据提取的现象\">#</a> 字典数据提取的现象:</h4>\n<p>把字典中国的一些类别数据，分别进行转换特征数据。</p>\n<blockquote>\n<p>文本特征抽取 Count</p>\n</blockquote>\n<h4 id=\"作用-对文本数据进行特征值化\"><a class=\"markdownIt-Anchor\" href=\"#作用-对文本数据进行特征值化\">#</a> 作用：对文本数据进行特征值化</h4>\n<h4 id=\"类-sklearnfeature_extractiontextcountvectorizer\"><a class=\"markdownIt-Anchor\" href=\"#类-sklearnfeature_extractiontextcountvectorizer\">#</a> 类: sklearn.feature_extraction.text.CountVectorizer</h4>\n<h4 id=\"api\"><a class=\"markdownIt-Anchor\" href=\"#api\">#</a> API</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 返回的是词频矩阵</span></span><br><span class=\"line\">CountVectorizer()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 统计所有文章中出现的词的位置标识，对应着列表中的单词(单个字母不统计,不支持中文,需要使用jieba分词)，</span></span><br><span class=\"line\">    <span class=\"comment\"># X 文本或者包含文本字符串的可迭代对象</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 返回sparse矩阵</span></span><br><span class=\"line\">    CountVectorizer.fit_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># X： array数据或者sparse矩阵</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值：转换之前的数据格式</span></span><br><span class=\"line\">    CountVetorizer.inverse_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 单词列表</span></span><br><span class=\"line\">    CountVetorizer.get_feature_names()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>文本特征抽取 tfidf</p>\n</blockquote>\n<p>主要思想是： 如果某个词或短语在一篇文章中出现的概率高，b 并且在其他文章中很少出现，则认为该词或者短语具有很好的类别区分能力，适合用来分类。</p>\n<p>TF-IDF 的作用：  用以评估一字词对于一个文件集成一个语料库中的其中一份文件的重要程度。</p>\n<p>tf (term frequency 词的频率)<br>\n idf (inverse document frequency 逆文档频率) log (总文档数据 / 该词出现的文档数量)</p>\n<p>tf * idf 这个值称为重要性。</p>\n<h4 id=\"类-sklearnfeature_extractiontexttfidfvectorizer\"><a class=\"markdownIt-Anchor\" href=\"#类-sklearnfeature_extractiontexttfidfvectorizer\">#</a> 类  <code>sklearn.feature_extraction.text.TfidfVectorizer </code></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 返回的是词频矩阵</span></span><br><span class=\"line\">CountVectorizer(stop_worlds=N)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 统计所有文章中出现的词的位置标识，对应着列表中的单词(单个字母不统计,不支持中文,需要使用jieba分词)，</span></span><br><span class=\"line\">    <span class=\"comment\"># X 文本或者包含文本字符串的可迭代对象</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 返回sparse矩阵</span></span><br><span class=\"line\">    CountVectorizer.fit_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># X： array数据或者sparse矩阵</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值：转换之前的数据格式</span></span><br><span class=\"line\">    CountVetorizer.inverse_transform(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 单词列表</span></span><br><span class=\"line\">    CountVetorizer.get_feature_names()</span><br></pre></td></tr></table></figure>\n<h4 id=\"为什么需要这\"><a class=\"markdownIt-Anchor\" href=\"#为什么需要这\">#</a> 为什么需要这</h4>\n<p>分类机器学习算法的应用基础。</p>\n<h4 id=\"应用\"><a class=\"markdownIt-Anchor\" href=\"#应用\">#</a> 应用</h4>\n<ul>\n<li>文本分类</li>\n<li>情感分析</li>\n</ul>\n<h4 id=\"数据的预处理\"><a class=\"markdownIt-Anchor\" href=\"#数据的预处理\">#</a> 数据的预处理</h4>\n<p>通过特定的统计方法 (数学方法), 将数据转换成算法要求的数据。</p>\n<h5 id=\"数据处理的方法\"><a class=\"markdownIt-Anchor\" href=\"#数据处理的方法\">#</a> 数据处理的方法</h5>\n<p>API:<br>\n 在 sklearn.preprocessing 中。</p>\n<ul>\n<li>\n<p>数值型数据:</p>\n<ul>\n<li>\n<p>标准缩放</p>\n<ul>\n<li>\n<p>归一化:<br>\n 通过对原始数据机型交换把数据映射到 (默认 [0,1]) 之间<br>\n<img data-src=\"./images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%BD%92%E4%B8%80%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"机器学习_归一归一化公式.png\"><br>\n <code> API: MinMaxScalar</code> <br>\n 实现归一化的步骤：</p>\n<ul>\n<li>实例化 <code>MinMaxScalar</code></li>\n<li>调用  <code>fit_transform</code>  进行转换</li>\n</ul>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mm = MinMaxScalar(feature_range=(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">data = mm.fit_transform([[],[],[]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>标准化<br>\n通过对原始数据进行变换把数据变换到均值为 0，标准差为 1 的范围内。<br>\n<img data-src=\"./images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.png\" alt=\"机器学习_标准化公式\"><br>\n对于归一化来讲，如果出现了异常点，影响了最大值和最小值，那么结果显然会发生改变.<br>\n 对标准化来讲，如果出现异常点，由于具有一定数据量，少量的异常点对平均值的影响并不大，从而方差改变较小。</p>\n<p><code>API: StandardScalar(....) </code> <br>\n标准化的步骤：</p>\n<ul>\n<li>实例化 StandardScalar</li>\n<li>调用 fit_transfrom</li>\n</ul>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#处理之后每列来说所有数据都聚集在均值0附近标准差差为1</span></span><br><span class=\"line\">StandardScalar(...)</span><br><span class=\"line\">    <span class=\"comment\"># X:numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 转换后的形状相同的array</span></span><br><span class=\"line\">    StandardScalar.fit_transfrom(X)</span><br><span class=\"line\">    <span class=\"comment\"># 原始数据中每列特征的平均值</span></span><br><span class=\"line\">    StandardScalar.mean_</span><br><span class=\"line\">    <span class=\"comment\"># 原始数据每列特征的方差</span></span><br><span class=\"line\">    StandardScalar.std_</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>缺失值<br>\n处理缺失值的方法:</p>\n<ul>\n<li>删除</li>\n<li>插补</li>\n</ul>\n</li>\n</ul>\n<p>插补使用的是:  <code>Imputer</code>  方法</p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 完成缺失值插补</span></span><br><span class=\"line\">Imputer(miss_values=<span class=\"string\">&#x27;NaN&#x27;</span>,strategy=<span class=\"string\">&#x27;mean&#x27;</span>,axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># X: numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 转换后的形状相同的array</span></span><br><span class=\"line\">    Imputer.fit_transform(X)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li>\n<p>类别型数据</p>\n<ul>\n<li>one-hot 编码</li>\n</ul>\n</li>\n<li>\n<p>时间类型</p>\n<ul>\n<li>时间的拆分</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数据的降维\"><a class=\"markdownIt-Anchor\" href=\"#数据的降维\">#</a> 数据的降维</h3>\n<p>降维指的是降的是特征的维度 (特征的数量)。<br>\n两种方式：特征选择 和 主成分分析</p>\n<h4 id=\"特征选择的原因\"><a class=\"markdownIt-Anchor\" href=\"#特征选择的原因\">#</a> 特征选择的原因</h4>\n<ul>\n<li>冗余：部分特征的相关度高，容易消耗计算机性能</li>\n<li>噪声：部分特征会对结果产生影响</li>\n</ul>\n<h4 id=\"特征选择是什么\"><a class=\"markdownIt-Anchor\" href=\"#特征选择是什么\">#</a> 特征选择是什么？</h4>\n<p>就是单纯的从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前后可以改变值，也可以不改变值。但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中一部分特征。<br>\n主要方法有:</p>\n<ul>\n<li>filter (过滤式): varianceThreshold</li>\n<li>Embedde (嵌入式): 正则化，决策树</li>\n<li>Wrapper (包裹式)</li>\n<li>神经网络</li>\n</ul>\n<p>API:  <code> sklearn.feature_selection.VarianceThreshold</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 删除所有低方差特征</span></span><br><span class=\"line\">VarianceThreshold(threhold=<span class=\"number\">0.0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># X: numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 训练集差异地域threshold的特征将被删除</span></span><br><span class=\"line\">    <span class=\"comment\"># 默认值是保留所有非零方差特征,即删除所有样本中具有相同值的特征</span></span><br><span class=\"line\">    Variance.fit_transform(X)</span><br></pre></td></tr></table></figure>\n<h3 id=\"pca-分析简化数据集的技术\"><a class=\"markdownIt-Anchor\" href=\"#pca-分析简化数据集的技术\">#</a> PCA 分析简化数据集的技术</h3>\n<p>目的是： 数据维度压缩，尽可能降低原数据的维度 (复杂度), 损失少量信息<br>\n作用：可以削减回归分析或者聚类分析中特征的数量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将数据分解为较低维数的空间</span></span><br><span class=\"line\">PCA(n_components=<span class=\"literal\">None</span>)</span><br><span class=\"line\">    <span class=\"comment\"># X: numpy array格式的数据[n_samples,n_features]</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回值: 转化后制定维度的array</span></span><br><span class=\"line\">    PCA.fit_transform(X)</span><br></pre></td></tr></table></figure>\n<h2 id=\"机器学习算法基础\"><a class=\"markdownIt-Anchor\" href=\"#机器学习算法基础\">#</a> 机器学习算法基础</h2>\n<h3 id=\"监督学习supervised-learnning\"><a class=\"markdownIt-Anchor\" href=\"#监督学习supervised-learnning\">#</a> 监督学习：supervised learnning</h3>\n<p>输入数据由输入特征值 和 目标值组成。</p>\n<p>目标值连续则是回归问题， 目标值离散则是分类问题。</p>\n<h3 id=\"无监督学习-unsupervised-learning\"><a class=\"markdownIt-Anchor\" href=\"#无监督学习-unsupervised-learning\">#</a> 无监督学习. unsupervised learning</h3>\n<p>输入数据并未进行标记，没有目标值。 =&gt; 聚类 (kmeans)</p>\n<h3 id=\"半监督学习-semi-supervised-learning\"><a class=\"markdownIt-Anchor\" href=\"#半监督学习-semi-supervised-learning\">#</a> 半监督学习： semi-supervised learning</h3>\n<p>训练集同时包含有标记样本数据 和 未标记样本数据</p>\n<h3 id=\"强化学习-reinforcement-learning\"><a class=\"markdownIt-Anchor\" href=\"#强化学习-reinforcement-learning\">#</a> 强化学习: reinforcement learning</h3>\n<p>本质是 make decisions 问题，即自动决策问题，并且可以做连续决策。</p>\n<p>没有训练数据，建立模型的时候人为设定好可以操作的规则，不断自我尝试，自己去探索。</p>\n<p>强化学习的目标就是获取更多的累计奖励。</p>\n<p>举个例子:</p>\n<p>小孩子想要走路，但是在这之前，他需要先站起来，站起来之后还要保持平衡，接下来就要先迈出一条腿，是左腿还是右腿，迈出一步还要迈出下一步。</p>\n<p>小孩子就是 agent，他试图通过 行动 (即行走) 来模型环境 (行走的表面) 并且从一个状态转变到另一个状态 (即他走的每一步)，当他完成了任务的子任务（即走了几步) 时，孩子收到奖励，并且当他不能走路时，就不会给奖励。</p>\n<p>主要包含四个元素: agent  行动 环境  奖励</p>\n<p><img data-src=\"./images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB.png\" alt=\"\"></p>\n<h2 id=\"最后\"><a class=\"markdownIt-Anchor\" href=\"#最后\">#</a> 最后</h2>\n<p>期望和你一起遇见更好的自己</p>\n<p><img data-src=\"/images/rocketmq/qrcode.jpg\" alt=\"\"></p>\n",
            "tags": [
                "MachineLearn"
            ]
        }
    ]
}